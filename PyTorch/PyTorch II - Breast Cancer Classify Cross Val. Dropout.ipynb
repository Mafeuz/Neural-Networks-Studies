{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch NN Studies Notebook II - Breast Cancer Classification (Cross Validation + Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "import skorch\n",
    "print(skorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetBinaryClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_path = 'C:/Users/Mafeus/Desktop/Curso PyTorch/Bases/Bases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>706.771388</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>4.304801</td>\n",
       "      <td>4.835984</td>\n",
       "      <td>7.489124</td>\n",
       "      <td>2.366459</td>\n",
       "      <td>16.965766</td>\n",
       "      <td>0.851112</td>\n",
       "      <td>...</td>\n",
       "      <td>315.194921</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>10.633281</td>\n",
       "      <td>25.259112</td>\n",
       "      <td>26.723742</td>\n",
       "      <td>8.745685</td>\n",
       "      <td>30.367174</td>\n",
       "      <td>1.964313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2430.243368</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>21.074558</td>\n",
       "      <td>26.827478</td>\n",
       "      <td>35.618994</td>\n",
       "      <td>16.155145</td>\n",
       "      <td>53.846023</td>\n",
       "      <td>7.103493</td>\n",
       "      <td>...</td>\n",
       "      <td>1655.459336</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>37.236433</td>\n",
       "      <td>96.473015</td>\n",
       "      <td>114.204035</td>\n",
       "      <td>39.465975</td>\n",
       "      <td>90.748044</td>\n",
       "      <td>14.464355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.760000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.210000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086410</td>\n",
       "      <td>0.065260</td>\n",
       "      <td>0.029580</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>...</td>\n",
       "      <td>13.180000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.064990</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.850000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095940</td>\n",
       "      <td>0.094620</td>\n",
       "      <td>0.063870</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>...</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.080060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.680000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.077260</td>\n",
       "      <td>0.203600</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>...</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.092110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9904.000000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9981.000000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>1058.000000</td>\n",
       "      <td>1252.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>544.000000</td>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        radius_mean   texture_mean   perimeter_mean    area_mean  \\\n",
       "count    569.000000     569.000000       569.000000   569.000000   \n",
       "mean     706.771388      19.289649        91.969033   654.889104   \n",
       "std     2430.243368       4.301036        24.298981   351.914129   \n",
       "min        7.760000       9.710000        43.790000   143.500000   \n",
       "25%       12.210000      16.170000        75.170000   420.300000   \n",
       "50%       13.850000      18.840000        86.240000   551.100000   \n",
       "75%       17.680000      21.800000       104.100000   782.700000   \n",
       "max     9904.000000      39.280000       188.500000  2501.000000   \n",
       "\n",
       "        smoothness_mean   compactness_mean   concavity_mean  \\\n",
       "count        569.000000         569.000000       569.000000   \n",
       "mean           4.304801           4.835984         7.489124   \n",
       "std           21.074558          26.827478        35.618994   \n",
       "min            0.052630           0.019380         0.000000   \n",
       "25%            0.086410           0.065260         0.029580   \n",
       "50%            0.095940           0.094620         0.063870   \n",
       "75%            0.106100           0.132500         0.142500   \n",
       "max          123.000000         277.000000       313.000000   \n",
       "\n",
       "       concave_points_mean   symmetry_mean   fractal_dimension_mean  ...  \\\n",
       "count           569.000000      569.000000               569.000000  ...   \n",
       "mean              2.366459       16.965766                 0.851112  ...   \n",
       "std              16.155145       53.846023                 7.103493  ...   \n",
       "min               0.000000        0.116700                 0.049960  ...   \n",
       "25%               0.020310        0.163400                 0.057800  ...   \n",
       "50%               0.033900        0.181400                 0.061660  ...   \n",
       "75%               0.077260        0.203600                 0.066400  ...   \n",
       "max             162.000000      304.000000                78.000000  ...   \n",
       "\n",
       "        radius_worst   texture_worst   perimeter_worst   area_worst  \\\n",
       "count     569.000000      569.000000        569.000000   569.000000   \n",
       "mean      315.194921       25.677223        107.261213   880.583128   \n",
       "std      1655.459336        6.146258         33.602542   569.356993   \n",
       "min         7.930000       12.020000         50.410000   185.200000   \n",
       "25%        13.180000       21.080000         84.110000   515.300000   \n",
       "50%        15.150000       25.410000         97.660000   686.500000   \n",
       "75%        19.850000       29.720000        125.400000  1084.000000   \n",
       "max      9981.000000       49.540000        251.200000  4254.000000   \n",
       "\n",
       "        smoothness_worst   compactness_worst   concavity_worst  \\\n",
       "count         569.000000          569.000000        569.000000   \n",
       "mean           10.633281           25.259112         26.723742   \n",
       "std            37.236433           96.473015        114.204035   \n",
       "min             0.071170            0.027290          0.000000   \n",
       "25%             0.117800            0.150700          0.116800   \n",
       "50%             0.133800            0.227900          0.249200   \n",
       "75%             0.150000            0.384200          0.431600   \n",
       "max           185.000000         1058.000000       1252.000000   \n",
       "\n",
       "        concave_points_worst   symmetry_worst   fractal_dimension_worst  \n",
       "count             569.000000       569.000000                569.000000  \n",
       "mean                8.745685        30.367174                  1.964313  \n",
       "std                39.465975        90.748044                 14.464355  \n",
       "min                 0.000000         0.156500                  0.055040  \n",
       "25%                 0.064990         0.254900                  0.071460  \n",
       "50%                 0.101500         0.288400                  0.080060  \n",
       "75%                 0.170800         0.331800                  0.092110  \n",
       "max               291.000000       544.000000                173.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data  = pd.read_csv(data_base_path + 'entradas_breast.csv')\n",
    "output_data = pd.read_csv(data_base_path + 'saidas_breast.csv')\n",
    "\n",
    "input_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  569.000000\n",
       "mean     0.627417\n",
       "std      0.483918\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(output_data['0']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x191269e0060>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(input_data, dtype=np.float32)\n",
    "Y = np.array(output_data, dtype=np.float32).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Model Build Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 30 -> 16 -> 16 -> 1\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight)\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "skorch_classifier = NeuralNetBinaryClassifier(module=torch_classifier, criterion=torch.nn.BCELoss,\n",
    "                                              optimizer=torch.optim.Adam, lr=0.001,\n",
    "                                              optimizer__weight_decay=0.0001, max_epochs=100, \n",
    "                                              batch_size=10, train_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:665: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_folds = np.zeros(n_samples, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1820  2.7376\n",
      "      2       37.1820  0.0856\n",
      "      3       37.1820  0.0878\n",
      "      4       37.1820  0.0931\n",
      "      5       37.1820  0.0944\n",
      "      6       37.1820  0.0944\n",
      "      7       37.1820  0.0983\n",
      "      8       37.1820  0.0983\n",
      "      9       37.1820  0.0957\n",
      "     10       37.1820  0.0962\n",
      "     11       37.1820  0.0996\n",
      "     12       37.1820  0.0899\n",
      "     13       37.1820  0.0861\n",
      "     14       37.1820  0.0886\n",
      "     15       37.1820  0.1206\n",
      "     16       37.1820  0.0966\n",
      "     17       37.1820  0.1160\n",
      "     18       37.1820  0.1017\n",
      "     19       37.1820  0.1007\n",
      "     20       37.1820  0.1016\n",
      "     21       37.1820  0.1060\n",
      "     22       37.1820  0.0995\n",
      "     23       37.1820  0.1023\n",
      "     24       37.1820  0.0993\n",
      "     25       13.6727  0.1031\n",
      "     26        0.5540  0.1087\n",
      "     27        0.5328  0.1120\n",
      "     28        0.5156  0.1147\n",
      "     29        0.4943  0.1227\n",
      "     30        0.4848  0.0915\n",
      "     31        0.4790  0.1171\n",
      "     32        0.4740  0.1005\n",
      "     33        0.4691  0.1183\n",
      "     34        0.4642  0.1419\n",
      "     35        0.4597  0.1536\n",
      "     36        0.4544  0.1132\n",
      "     37        0.4487  0.1182\n",
      "     38        0.4430  0.1337\n",
      "     39        0.4379  0.1094\n",
      "     40        0.4329  0.0926\n",
      "     41        0.4265  0.1089\n",
      "     42        0.4206  0.1357\n",
      "     43        0.4143  0.0990\n",
      "     44        0.4070  0.1012\n",
      "     45        0.3962  0.1516\n",
      "     46        0.3859  0.1019\n",
      "     47        0.3759  0.1505\n",
      "     48        0.3651  0.0952\n",
      "     49        0.3545  0.1018\n",
      "     50        0.3432  0.0885\n",
      "     51        0.3335  0.1004\n",
      "     52        0.3212  0.1185\n",
      "     53        0.3167  0.0999\n",
      "     54        0.3044  0.1107\n",
      "     55        0.3013  0.0990\n",
      "     56        0.2891  0.1201\n",
      "     57        0.2826  0.1097\n",
      "     58        0.2738  0.1057\n",
      "     59        0.2684  0.0998\n",
      "     60        0.2631  0.0963\n",
      "     61        0.2546  0.1068\n",
      "     62        0.2474  0.1390\n",
      "     63        0.2429  0.1071\n",
      "     64        0.2574  0.1335\n",
      "     65        0.2336  0.1257\n",
      "     66        0.2266  0.0991\n",
      "     67        0.2207  0.1050\n",
      "     68        0.2243  0.1177\n",
      "     69        0.2130  0.1362\n",
      "     70        0.2108  0.1177\n",
      "     71        0.2134  0.1077\n",
      "     72        0.2055  0.0975\n",
      "     73        0.2008  0.1225\n",
      "     74        0.2015  0.0971\n",
      "     75        0.1988  0.1010\n",
      "     76        0.1958  0.1047\n",
      "     77        0.1954  0.1031\n",
      "     78        0.1922  0.1005\n",
      "     79        0.1864  0.1361\n",
      "     80        0.1879  0.0976\n",
      "     81        0.1761  0.1105\n",
      "     82        0.1734  0.0925\n",
      "     83        0.1766  0.1104\n",
      "     84        0.1712  0.1254\n",
      "     85        0.1601  0.1042\n",
      "     86        0.1748  0.1001\n",
      "     87        0.1775  0.1030\n",
      "     88        0.1628  0.1022\n",
      "     89        0.1553  0.1019\n",
      "     90        0.1536  0.1043\n",
      "     91        0.1551  0.1199\n",
      "     92        0.1682  0.1032\n",
      "     93        0.1753  0.1201\n",
      "     94        0.1628  0.1195\n",
      "     95        0.1647  0.0872\n",
      "     96        0.1570  0.1035\n",
      "     97        0.1488  0.1210\n",
      "     98        0.1500  0.1158\n",
      "     99        0.1458  0.1034\n",
      "    100        0.1401  0.1165\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1820  0.1094\n",
      "      2       37.1820  0.1065\n",
      "      3       37.1820  0.1247\n",
      "      4       37.1820  0.1187\n",
      "      5       37.1820  0.0997\n",
      "      6       37.1820  0.0995\n",
      "      7       37.1820  0.1151\n",
      "      8       37.1820  0.1035\n",
      "      9       37.1820  0.1297\n",
      "     10       37.1820  0.1037\n",
      "     11       37.1820  0.1009\n",
      "     12       37.1820  0.1093\n",
      "     13       37.1820  0.1077\n",
      "     14       37.1820  0.1058\n",
      "     15       37.1820  0.1001\n",
      "     16       37.1820  0.0860\n",
      "     17       37.1820  0.1078\n",
      "     18       37.1820  0.1283\n",
      "     19       37.1820  0.1177\n",
      "     20       37.1820  0.1127\n",
      "     21       37.1820  0.1137\n",
      "     22       37.1820  0.1217\n",
      "     23       37.1820  0.1117\n",
      "     24       37.1820  0.1349\n",
      "     25       18.2752  0.1037\n",
      "     26        0.5688  0.1191\n",
      "     27        0.5527  0.1349\n",
      "     28        0.5499  0.1022\n",
      "     29        0.5490  0.1017\n",
      "     30        0.5457  0.1216\n",
      "     31        0.5508  0.1105\n",
      "     32        0.5391  0.0902\n",
      "     33        0.5351  0.1116\n",
      "     34        0.5271  0.1261\n",
      "     35        0.5229  0.1184\n",
      "     36        0.5015  0.1391\n",
      "     37        0.4818  0.1137\n",
      "     38        0.4613  0.1127\n",
      "     39        0.4393  0.1267\n",
      "     40        0.4157  0.1685\n",
      "     41        0.3978  0.1132\n",
      "     42        0.3834  0.1333\n",
      "     43        0.3687  0.1461\n",
      "     44        0.3535  0.0914\n",
      "     45        0.3389  0.1028\n",
      "     46        0.3239  0.1195\n",
      "     47        0.3073  0.1182\n",
      "     48        0.2803  0.1172\n",
      "     49        0.2494  0.1176\n",
      "     50        0.2273  0.1177\n",
      "     51        0.2131  0.1179\n",
      "     52        0.2028  0.1172\n",
      "     53        0.1954  0.1031\n",
      "     54        0.1906  0.1176\n",
      "     55        0.1886  0.1310\n",
      "     56        0.1844  0.1064\n",
      "     57        0.1830  0.1172\n",
      "     58        0.1767  0.1194\n",
      "     59        0.1726  0.1613\n",
      "     60        0.1689  0.1009\n",
      "     61        0.1665  0.1215\n",
      "     62        0.1630  0.1005\n",
      "     63        0.1624  0.0987\n",
      "     64        0.1561  0.1227\n",
      "     65        0.1544  0.1157\n",
      "     66        0.1548  0.1107\n",
      "     67        0.1478  0.0942\n",
      "     68        0.1472  0.1346\n",
      "     69        0.1457  0.1132\n",
      "     70        0.1400  0.1218\n",
      "     71        0.1388  0.1301\n",
      "     72        0.1369  0.1217\n",
      "     73        0.1353  0.1147\n",
      "     74        0.1347  0.0989\n",
      "     75        0.1317  0.1127\n",
      "     76        0.1308  0.1245\n",
      "     77        0.1281  0.1093\n",
      "     78        0.1280  0.1087\n",
      "     79        0.1269  0.1046\n",
      "     80        0.1262  0.1006\n",
      "     81        0.1257  0.1066\n",
      "     82        0.1233  0.1037\n",
      "     83        0.1234  0.1158\n",
      "     84        0.1215  0.1065\n",
      "     85        0.1213  0.1008\n",
      "     86        0.1191  0.1032\n",
      "     87        0.1181  0.1044\n",
      "     88        0.1185  0.1211\n",
      "     89        0.1192  0.1031\n",
      "     90        0.1179  0.1031\n",
      "     91        0.1130  0.1114\n",
      "     92        0.1114  0.1023\n",
      "     93        0.1122  0.1187\n",
      "     94        0.1080  0.1175\n",
      "     95        0.1112  0.1187\n",
      "     96        0.1066  0.1183\n",
      "     97        0.1103  0.1046\n",
      "     98        0.1084  0.1034\n",
      "     99        0.1095  0.1025\n",
      "    100        0.1081  0.1038\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0990\n",
      "      2       37.3047  0.1138\n",
      "      3       37.3047  0.1133\n",
      "      4       37.3047  0.1190\n",
      "      5       37.3047  0.1047\n",
      "      6       37.3047  0.0998\n",
      "      7       37.3047  0.1145\n",
      "      8       37.3047  0.1013\n",
      "      9       37.3047  0.1018\n",
      "     10       37.3047  0.1017\n",
      "     11       37.3047  0.1006\n",
      "     12       37.3047  0.0990\n",
      "     13       37.3047  0.1037\n",
      "     14       37.3047  0.1013\n",
      "     15       37.3047  0.0993\n",
      "     16       37.3047  0.1165\n",
      "     17       37.3047  0.1030\n",
      "     18       37.3047  0.0991\n",
      "     19       37.3047  0.1035\n",
      "     20       37.3047  0.1008\n",
      "     21       37.3047  0.1014\n",
      "     22       37.3047  0.0986\n",
      "     23       37.3047  0.1184\n",
      "     24       37.3047  0.1016\n",
      "     25       37.3047  0.1155\n",
      "     26       12.0554  0.1027\n",
      "     27        0.6218  0.1188\n",
      "     28        0.5598  0.1166\n",
      "     29        0.5421  0.1173\n",
      "     30        0.5217  0.1322\n",
      "     31        0.5103  0.1190\n",
      "     32        0.5066  0.1180\n",
      "     33        0.4987  0.1168\n",
      "     34        0.4896  0.1161\n",
      "     35        0.4823  0.1020\n",
      "     36        0.4724  0.1115\n",
      "     37        0.4708  0.1183\n",
      "     38        0.4657  0.1054\n",
      "     39        0.4598  0.1227\n",
      "     40        0.4518  0.0838\n",
      "     41        0.4435  0.1046\n",
      "     42        0.4372  0.1313\n",
      "     43        0.4256  0.1426\n",
      "     44        0.4203  0.1157\n",
      "     45        0.4111  0.1190\n",
      "     46        0.3965  0.1165\n",
      "     47        0.4017  0.1008\n",
      "     48        0.3832  0.1003\n",
      "     49        0.3736  0.1168\n",
      "     50        0.3779  0.1033\n",
      "     51        0.3566  0.1127\n",
      "     52        0.3550  0.1020\n",
      "     53        0.3428  0.1049\n",
      "     54        0.3252  0.1013\n",
      "     55        0.3170  0.1025\n",
      "     56        0.3042  0.1198\n",
      "     57        0.2979  0.1006\n",
      "     58        0.2971  0.1026\n",
      "     59        0.2881  0.1015\n",
      "     60        0.2871  0.1036\n",
      "     61        0.2770  0.1032\n",
      "     62        0.2702  0.1376\n",
      "     63        0.2754  0.1145\n",
      "     64        0.2697  0.1040\n",
      "     65        0.2695  0.1002\n",
      "     66        0.2597  0.1036\n",
      "     67        0.2585  0.1425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     68        0.2540  0.1194\n",
      "     69        0.2561  0.1032\n",
      "     70        0.2462  0.0874\n",
      "     71        0.2495  0.1025\n",
      "     72        0.2406  0.1343\n",
      "     73        0.2395  0.0965\n",
      "     74        0.2381  0.1016\n",
      "     75        0.2384  0.1192\n",
      "     76        0.2341  0.0877\n",
      "     77        0.2350  0.1188\n",
      "     78        0.2299  0.1032\n",
      "     79        0.2284  0.1008\n",
      "     80        0.2224  0.1038\n",
      "     81        0.2209  0.1025\n",
      "     82        0.2182  0.1028\n",
      "     83        0.2167  0.1032\n",
      "     84        0.2114  0.1026\n",
      "     85        0.2118  0.0901\n",
      "     86        0.2079  0.1041\n",
      "     87        0.2095  0.1042\n",
      "     88        0.2069  0.0869\n",
      "     89        0.2078  0.1169\n",
      "     90        0.2030  0.1036\n",
      "     91        0.2033  0.1043\n",
      "     92        0.1995  0.1036\n",
      "     93        0.1984  0.1022\n",
      "     94        0.1988  0.1039\n",
      "     95        0.1927  0.1037\n",
      "     96        0.1987  0.1036\n",
      "     97        0.1910  0.1027\n",
      "     98        0.1879  0.1023\n",
      "     99        0.1882  0.1043\n",
      "    100        0.1836  0.1043\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0842\n",
      "      2       37.3047  0.1006\n",
      "      3       37.3047  0.1143\n",
      "      4       37.3047  0.0861\n",
      "      5       37.3047  0.1092\n",
      "      6       37.3047  0.1077\n",
      "      7       37.3047  0.1161\n",
      "      8       37.3047  0.0991\n",
      "      9       37.3047  0.1463\n",
      "     10       37.3047  0.1071\n",
      "     11       37.3047  0.0992\n",
      "     12       37.3047  0.1324\n",
      "     13       37.3047  0.1032\n",
      "     14       37.3047  0.1002\n",
      "     15       37.3047  0.1007\n",
      "     16       37.3047  0.1013\n",
      "     17       37.3047  0.1006\n",
      "     18       37.3047  0.0995\n",
      "     19       37.3047  0.1011\n",
      "     20       37.3047  0.1003\n",
      "     21       37.3047  0.1409\n",
      "     22       37.3047  0.1316\n",
      "     23       37.3047  0.0922\n",
      "     24       37.3047  0.1010\n",
      "     25       37.3047  0.0987\n",
      "     26       37.3047  0.1297\n",
      "     27       37.3047  0.1287\n",
      "     28       37.3047  0.0965\n",
      "     29       37.3047  0.1170\n",
      "     30       37.3047  0.1224\n",
      "     31       37.3047  0.1247\n",
      "     32       37.3047  0.1177\n",
      "     33       37.3047  0.0989\n",
      "     34       37.3047  0.1027\n",
      "     35       37.3047  0.1339\n",
      "     36       37.3047  0.1037\n",
      "     37       37.3047  0.0988\n",
      "     38       37.3047  0.1023\n",
      "     39       37.3047  0.1155\n",
      "     40       37.3047  0.1277\n",
      "     41       37.3047  0.0961\n",
      "     42       37.3047  0.1390\n",
      "     43       37.3047  0.1219\n",
      "     44       37.3047  0.1259\n",
      "     45       37.3047  0.1211\n",
      "     46       37.3047  0.1168\n",
      "     47       37.3047  0.1346\n",
      "     48       37.3047  0.0970\n",
      "     49       37.3047  0.1279\n",
      "     50       37.3047  0.1168\n",
      "     51       37.3047  0.0926\n",
      "     52       37.3047  0.1013\n",
      "     53       37.3047  0.1035\n",
      "     54       37.3047  0.1010\n",
      "     55       37.3047  0.0876\n",
      "     56       37.3047  0.1107\n",
      "     57       37.3047  0.1027\n",
      "     58       37.3047  0.1047\n",
      "     59       37.3047  0.1003\n",
      "     60       37.3047  0.1031\n",
      "     61       37.3047  0.0959\n",
      "     62       37.3047  0.0982\n",
      "     63       37.3047  0.1172\n",
      "     64       37.3047  0.1621\n",
      "     65       37.3047  0.1341\n",
      "     66       37.3047  0.1870\n",
      "     67       37.3047  0.1523\n",
      "     68       37.3047  0.1122\n",
      "     69       37.3047  0.1114\n",
      "     70       37.3047  0.1023\n",
      "     71       37.3047  0.1023\n",
      "     72       37.3047  0.1017\n",
      "     73       37.3047  0.1029\n",
      "     74       37.3047  0.1012\n",
      "     75       37.3047  0.1158\n",
      "     76       37.3047  0.1024\n",
      "     77       37.3047  0.1014\n",
      "     78       37.3047  0.1006\n",
      "     79       37.3047  0.1043\n",
      "     80       37.3047  0.1032\n",
      "     81       37.3047  0.1174\n",
      "     82       37.3047  0.0872\n",
      "     83       37.3047  0.1038\n",
      "     84       37.3047  0.1008\n",
      "     85       37.3047  0.1038\n",
      "     86       37.3047  0.1027\n",
      "     87       37.3047  0.1123\n",
      "     88       37.3047  0.1057\n",
      "     89       37.3047  0.0989\n",
      "     90       37.3047  0.1021\n",
      "     91       37.3047  0.1184\n",
      "     92       37.3047  0.1107\n",
      "     93       37.3047  0.1160\n",
      "     94       37.3047  0.1190\n",
      "     95       37.3047  0.1385\n",
      "     96       37.3047  0.0998\n",
      "     97       37.3047  0.1054\n",
      "     98       37.3047  0.1027\n",
      "     99       37.3047  0.1034\n",
      "    100       37.3047  0.1021\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.1247\n",
      "      2       37.3047  0.1028\n",
      "      3       37.3047  0.1016\n",
      "      4       37.3047  0.1012\n",
      "      5       37.3047  0.1154\n",
      "      6       37.3047  0.1003\n",
      "      7       37.3047  0.1004\n",
      "      8       37.3047  0.1010\n",
      "      9       37.3047  0.1480\n",
      "     10       37.3047  0.1031\n",
      "     11       37.3047  0.1110\n",
      "     12       37.3047  0.1023\n",
      "     13       37.3047  0.1175\n",
      "     14       37.3047  0.1012\n",
      "     15       37.3047  0.1011\n",
      "     16       37.3047  0.1396\n",
      "     17       37.3047  0.1182\n",
      "     18       37.3047  0.1006\n",
      "     19       37.3047  0.1014\n",
      "     20       37.3047  0.1012\n",
      "     21       37.3047  0.1005\n",
      "     22       37.3047  0.1020\n",
      "     23       37.3047  0.0984\n",
      "     24       37.3047  0.1032\n",
      "     25       37.3047  0.1167\n",
      "     26       37.3047  0.0999\n",
      "     27       15.4257  0.1149\n",
      "     28        0.5390  0.1416\n",
      "     29        0.5115  0.0980\n",
      "     30        0.4965  0.1196\n",
      "     31        0.4822  0.1020\n",
      "     32        0.4651  0.1010\n",
      "     33        0.4528  0.1247\n",
      "     34        0.4388  0.1097\n",
      "     35        0.4185  0.0956\n",
      "     36        0.4334  0.1186\n",
      "     37        0.4169  0.1012\n",
      "     38        0.4174  0.1009\n",
      "     39        0.3996  0.1165\n",
      "     40        0.3995  0.1127\n",
      "     41        0.3831  0.1076\n",
      "     42        0.3797  0.1237\n",
      "     43        0.3698  0.1067\n",
      "     44        0.3597  0.1113\n",
      "     45        0.3801  0.1184\n",
      "     46        0.3474  0.1011\n",
      "     47        0.3430  0.1168\n",
      "     48        0.3401  0.1021\n",
      "     49        0.3382  0.1000\n",
      "     50        0.3214  0.1201\n",
      "     51        0.3104  0.1245\n",
      "     52        0.3056  0.1013\n",
      "     53        0.2987  0.1197\n",
      "     54        0.2946  0.1117\n",
      "     55        0.2853  0.0912\n",
      "     56        0.2775  0.1166\n",
      "     57        0.2720  0.1055\n",
      "     58        0.2716  0.1260\n",
      "     59        0.2610  0.1310\n",
      "     60        0.2551  0.1110\n",
      "     61        0.2510  0.1180\n",
      "     62        0.2413  0.1174\n",
      "     63        0.2371  0.1409\n",
      "     64        0.2350  0.1086\n",
      "     65        0.2336  0.1110\n",
      "     66        0.2307  0.1045\n",
      "     67        0.2123  0.1136\n",
      "     68        0.2131  0.1057\n",
      "     69        0.2109  0.1023\n",
      "     70        0.2141  0.1151\n",
      "     71        0.2053  0.1048\n",
      "     72        0.2084  0.1188\n",
      "     73        0.2049  0.1017\n",
      "     74        0.2061  0.1212\n",
      "     75        0.1985  0.1058\n",
      "     76        0.1930  0.1143\n",
      "     77        0.1920  0.1025\n",
      "     78        0.1909  0.1041\n",
      "     79        0.1878  0.1033\n",
      "     80        0.1925  0.1020\n",
      "     81        0.1841  0.1032\n",
      "     82        0.1774  0.1057\n",
      "     83        0.1817  0.1027\n",
      "     84        0.1755  0.1175\n",
      "     85        0.1772  0.1214\n",
      "     86        0.1758  0.1023\n",
      "     87        0.1710  0.1177\n",
      "     88        0.1733  0.1035\n",
      "     89        0.1695  0.1055\n",
      "     90        0.1679  0.1194\n",
      "     91        0.1747  0.1037\n",
      "     92        0.1604  0.1103\n",
      "     93        0.1649  0.1230\n",
      "     94        0.1597  0.1137\n",
      "     95        0.1634  0.1183\n",
      "     96        0.1633  0.1107\n",
      "     97        0.1623  0.1149\n",
      "     98        0.1552  0.1260\n",
      "     99        0.1584  0.1023\n",
      "    100        0.1532  0.1269\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0906\n",
      "      2       37.3047  0.1247\n",
      "      3       37.3047  0.1246\n",
      "      4       37.3047  0.1302\n",
      "      5       37.3047  0.1309\n",
      "      6       37.3047  0.2012\n",
      "      7       37.3047  0.1091\n",
      "      8       37.3047  0.0851\n",
      "      9       37.3047  0.0990\n",
      "     10       37.3047  0.1248\n",
      "     11       37.3047  0.1197\n",
      "     12       37.3047  0.1077\n",
      "     13       37.3047  0.0853\n",
      "     14       37.3047  0.1027\n",
      "     15       37.3047  0.0923\n",
      "     16       37.3047  0.0896\n",
      "     17       37.3047  0.1624\n",
      "     18       37.3047  0.1506\n",
      "     19       37.3047  0.1346\n",
      "     20       37.3047  0.1287\n",
      "     21       37.3047  0.0847\n",
      "     22       37.3047  0.0937\n",
      "     23       35.1446  0.1077\n",
      "     24        0.5570  0.0985\n",
      "     25        0.5403  0.1010\n",
      "     26        0.4892  0.1015\n",
      "     27        0.4658  0.1113\n",
      "     28        0.4427  0.1326\n",
      "     29        0.4252  0.0928\n",
      "     30        0.4094  0.1040\n",
      "     31        0.4011  0.0921\n",
      "     32        0.3859  0.1166\n",
      "     33        0.3755  0.0882\n",
      "     34        0.3605  0.1310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35        0.3451  0.1087\n",
      "     36        0.3408  0.0968\n",
      "     37        0.3301  0.0995\n",
      "     38        0.3217  0.1279\n",
      "     39        0.3137  0.1289\n",
      "     40        0.3026  0.0860\n",
      "     41        0.2972  0.1181\n",
      "     42        0.2913  0.1297\n",
      "     43        0.2876  0.0927\n",
      "     44        0.2823  0.1266\n",
      "     45        0.2755  0.1008\n",
      "     46        0.2685  0.1103\n",
      "     47        0.2630  0.0912\n",
      "     48        0.2573  0.1109\n",
      "     49        0.2530  0.0909\n",
      "     50        0.2437  0.0936\n",
      "     51        0.2378  0.0934\n",
      "     52        0.2276  0.1133\n",
      "     53        0.2234  0.1043\n",
      "     54        0.2159  0.1033\n",
      "     55        0.2126  0.0890\n",
      "     56        0.2068  0.1007\n",
      "     57        0.2055  0.1217\n",
      "     58        0.1999  0.0951\n",
      "     59        0.2033  0.0954\n",
      "     60        0.1994  0.0850\n",
      "     61        0.1957  0.1002\n",
      "     62        0.1930  0.1177\n",
      "     63        0.1903  0.1148\n",
      "     64        0.1919  0.1164\n",
      "     65        0.1890  0.0817\n",
      "     66        0.1890  0.1456\n",
      "     67        0.1865  0.0858\n",
      "     68        0.1853  0.0885\n",
      "     69        0.1735  0.1004\n",
      "     70        0.1793  0.0909\n",
      "     71        0.1735  0.0926\n",
      "     72        0.1731  0.0950\n",
      "     73        0.1691  0.1105\n",
      "     74        0.1663  0.0900\n",
      "     75        0.1629  0.0950\n",
      "     76        0.1592  0.0950\n",
      "     77        0.1612  0.0980\n",
      "     78        0.1553  0.0958\n",
      "     79        0.1576  0.0991\n",
      "     80        0.1577  0.1112\n",
      "     81        0.1587  0.0992\n",
      "     82        0.1568  0.0927\n",
      "     83        0.1585  0.0860\n",
      "     84        0.1594  0.0987\n",
      "     85        0.1542  0.0979\n",
      "     86        0.1557  0.0934\n",
      "     87        0.1552  0.0994\n",
      "     88        0.1504  0.0990\n",
      "     89        0.1491  0.0846\n",
      "     90        0.1555  0.1047\n",
      "     91        0.1489  0.1177\n",
      "     92        0.1462  0.0905\n",
      "     93        0.1432  0.0993\n",
      "     94        0.1481  0.1267\n",
      "     95        0.1437  0.1067\n",
      "     96        0.1423  0.1191\n",
      "     97        0.1496  0.1022\n",
      "     98        0.1432  0.0873\n",
      "     99        0.1444  0.0997\n",
      "    100        0.1451  0.0963\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0781\n",
      "      2       37.3047  0.0963\n",
      "      3       37.3047  0.0949\n",
      "      4       37.3047  0.0869\n",
      "      5       37.3047  0.1098\n",
      "      6       37.3047  0.1187\n",
      "      7       37.3047  0.0884\n",
      "      8       37.3047  0.0989\n",
      "      9       37.3047  0.1256\n",
      "     10       37.3047  0.0792\n",
      "     11       37.3047  0.0842\n",
      "     12       37.3047  0.0868\n",
      "     13       37.3047  0.0930\n",
      "     14       37.3047  0.0973\n",
      "     15       37.3047  0.0836\n",
      "     16       37.3047  0.0926\n",
      "     17       37.3047  0.1130\n",
      "     18       37.3047  0.1227\n",
      "     19       37.3047  0.0894\n",
      "     20       37.3047  0.0850\n",
      "     21       37.3047  0.1133\n",
      "     22       37.3047  0.0957\n",
      "     23       37.3047  0.0973\n",
      "     24       37.3047  0.1089\n",
      "     25       16.2499  0.0924\n",
      "     26        0.5846  0.1035\n",
      "     27        0.5754  0.1142\n",
      "     28        0.5617  0.1016\n",
      "     29        0.5579  0.1050\n",
      "     30        0.5474  0.1270\n",
      "     31        0.4928  0.1050\n",
      "     32        0.4689  0.1136\n",
      "     33        0.4484  0.1200\n",
      "     34        0.4265  0.1159\n",
      "     35        0.4068  0.1500\n",
      "     36        0.3939  0.1146\n",
      "     37        0.3687  0.1021\n",
      "     38        0.3532  0.1033\n",
      "     39        0.3287  0.1187\n",
      "     40        0.3233  0.1015\n",
      "     41        0.3063  0.1007\n",
      "     42        0.3025  0.1117\n",
      "     43        0.2931  0.1087\n",
      "     44        0.2888  0.1089\n",
      "     45        0.2871  0.1041\n",
      "     46        0.2904  0.1257\n",
      "     47        0.2862  0.1247\n",
      "     48        0.2845  0.1087\n",
      "     49        0.2839  0.0995\n",
      "     50        0.2819  0.1139\n",
      "     51        0.2807  0.1127\n",
      "     52        0.2810  0.1079\n",
      "     53        0.2764  0.1102\n",
      "     54        0.2773  0.0959\n",
      "     55        0.2768  0.1188\n",
      "     56        0.2702  0.0986\n",
      "     57        0.2594  0.1287\n",
      "     58        0.2551  0.0927\n",
      "     59        0.2571  0.1202\n",
      "     60        0.2504  0.1007\n",
      "     61        0.2424  0.1357\n",
      "     62        0.2408  0.1103\n",
      "     63        0.2354  0.1177\n",
      "     64        0.2337  0.1223\n",
      "     65        0.2266  0.0914\n",
      "     66        0.2274  0.1346\n",
      "     67        0.2218  0.1217\n",
      "     68        0.2240  0.1178\n",
      "     69        0.2147  0.1143\n",
      "     70        0.2121  0.1118\n",
      "     71        0.2072  0.1135\n",
      "     72        0.2065  0.1078\n",
      "     73        0.2076  0.1256\n",
      "     74        0.2061  0.1167\n",
      "     75        0.1971  0.1336\n",
      "     76        0.1960  0.1316\n",
      "     77        0.2005  0.1715\n",
      "     78        0.1976  0.1009\n",
      "     79        0.1981  0.1189\n",
      "     80        0.1963  0.1267\n",
      "     81        0.1961  0.0935\n",
      "     82        0.2015  0.1102\n",
      "     83        0.1968  0.1077\n",
      "     84        0.2009  0.0919\n",
      "     85        0.1930  0.0928\n",
      "     86        0.2012  0.0921\n",
      "     87        0.1968  0.1446\n",
      "     88        0.1931  0.1200\n",
      "     89        0.1900  0.0884\n",
      "     90        0.1894  0.0923\n",
      "     91        0.1939  0.0931\n",
      "     92        0.1977  0.0903\n",
      "     93        0.1855  0.0971\n",
      "     94        0.1939  0.0917\n",
      "     95        0.1828  0.0864\n",
      "     96        0.1803  0.0989\n",
      "     97        0.1860  0.1080\n",
      "     98        0.1821  0.0986\n",
      "     99        0.1972  0.0867\n",
      "    100        0.1821  0.1017\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0781\n",
      "      2       37.2320  0.0977\n",
      "      3       37.2320  0.0868\n",
      "      4       37.2320  0.0970\n",
      "      5       37.2320  0.0857\n",
      "      6       37.2320  0.0847\n",
      "      7       37.2320  0.0781\n",
      "      8       37.2320  0.0882\n",
      "      9       37.2320  0.0871\n",
      "     10       37.2320  0.0968\n",
      "     11       37.2320  0.0877\n",
      "     12       37.2320  0.0915\n",
      "     13       37.2320  0.0830\n",
      "     14       37.2320  0.0945\n",
      "     15       37.2320  0.0968\n",
      "     16       37.2320  0.0865\n",
      "     17       37.2320  0.0867\n",
      "     18       37.2320  0.0890\n",
      "     19       37.2320  0.0924\n",
      "     20       37.2320  0.0993\n",
      "     21       37.2320  0.0977\n",
      "     22       18.0027  0.1225\n",
      "     23        0.6662  0.0981\n",
      "     24        0.5660  0.0978\n",
      "     25        0.5221  0.0956\n",
      "     26        0.5008  0.0984\n",
      "     27        0.4837  0.0951\n",
      "     28        0.4692  0.0940\n",
      "     29        0.4494  0.1256\n",
      "     30        0.4316  0.0990\n",
      "     31        0.4069  0.1006\n",
      "     32        0.3906  0.0974\n",
      "     33        0.3775  0.1002\n",
      "     34        0.3636  0.1363\n",
      "     35        0.3564  0.1021\n",
      "     36        0.3474  0.0933\n",
      "     37        0.3413  0.1130\n",
      "     38        0.3329  0.0986\n",
      "     39        0.3263  0.1289\n",
      "     40        0.3165  0.1033\n",
      "     41        0.3126  0.1004\n",
      "     42        0.3075  0.0963\n",
      "     43        0.3058  0.0928\n",
      "     44        0.3043  0.0897\n",
      "     45        0.2976  0.1037\n",
      "     46        0.2922  0.0885\n",
      "     47        0.2930  0.0883\n",
      "     48        0.2897  0.0937\n",
      "     49        0.2850  0.1029\n",
      "     50        0.2834  0.1043\n",
      "     51        0.2781  0.0921\n",
      "     52        0.2727  0.1027\n",
      "     53        0.2704  0.0983\n",
      "     54        0.2695  0.0887\n",
      "     55        0.2648  0.1153\n",
      "     56        0.2617  0.1086\n",
      "     57        0.2592  0.0976\n",
      "     58        0.2588  0.0901\n",
      "     59        0.2554  0.0956\n",
      "     60        0.2491  0.0997\n",
      "     61        0.2485  0.0909\n",
      "     62        0.2475  0.0998\n",
      "     63        0.2460  0.1085\n",
      "     64        0.2458  0.0910\n",
      "     65        0.2424  0.0922\n",
      "     66        0.2395  0.1329\n",
      "     67        0.2358  0.0905\n",
      "     68        0.2369  0.0990\n",
      "     69        0.2340  0.0973\n",
      "     70        0.2202  0.0997\n",
      "     71        0.2334  0.1185\n",
      "     72        0.2287  0.0848\n",
      "     73        0.2217  0.1147\n",
      "     74        0.2197  0.0955\n",
      "     75        0.2108  0.0981\n",
      "     76        0.2148  0.0970\n",
      "     77        0.2126  0.0968\n",
      "     78        0.2167  0.1057\n",
      "     79        0.2074  0.1049\n",
      "     80        0.1973  0.1043\n",
      "     81        0.1991  0.0905\n",
      "     82        0.1978  0.0886\n",
      "     83        0.1924  0.0890\n",
      "     84        0.1883  0.0896\n",
      "     85        0.1865  0.1000\n",
      "     86        0.1868  0.1034\n",
      "     87        0.1867  0.1038\n",
      "     88        0.1846  0.1276\n",
      "     89        0.1829  0.0901\n",
      "     90        0.1780  0.1019\n",
      "     91        0.1828  0.0898\n",
      "     92        0.1838  0.0901\n",
      "     93        0.1760  0.0941\n",
      "     94        0.1726  0.1097\n",
      "     95        0.1703  0.1013\n",
      "     96        0.1719  0.1060\n",
      "     97        0.1688  0.0906\n",
      "     98        0.1681  0.1005\n",
      "     99        0.1600  0.0977\n",
      "    100        0.1572  0.0928\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2       37.2320  0.1023\n",
      "      3       37.2320  0.0944\n",
      "      4       37.2320  0.0893\n",
      "      5       37.2320  0.1056\n",
      "      6       37.2320  0.0922\n",
      "      7       37.2320  0.0984\n",
      "      8       37.2320  0.0925\n",
      "      9       37.2320  0.0934\n",
      "     10       37.2320  0.0910\n",
      "     11       37.2320  0.0844\n",
      "     12       37.2320  0.1095\n",
      "     13       37.2320  0.0982\n",
      "     14       37.2320  0.0967\n",
      "     15       37.2320  0.0957\n",
      "     16       37.2320  0.0961\n",
      "     17       37.2320  0.0954\n",
      "     18       37.2320  0.0991\n",
      "     19       37.2320  0.0964\n",
      "     20       37.2320  0.0825\n",
      "     21       37.2320  0.0874\n",
      "     22       37.2320  0.0922\n",
      "     23       37.2320  0.0846\n",
      "     24       37.2320  0.0918\n",
      "     25       37.2320  0.0835\n",
      "     26       37.2320  0.1080\n",
      "     27       37.2320  0.0955\n",
      "     28       37.2320  0.0864\n",
      "     29       37.2320  0.0849\n",
      "     30       37.2320  0.0873\n",
      "     31       37.2320  0.1068\n",
      "     32       37.2320  0.0996\n",
      "     33       37.2320  0.0938\n",
      "     34       37.2320  0.0912\n",
      "     35       37.2320  0.0903\n",
      "     36       37.2320  0.0855\n",
      "     37       37.2320  0.0868\n",
      "     38       37.2320  0.0915\n",
      "     39       37.2320  0.0971\n",
      "     40       37.2320  0.0944\n",
      "     41       37.2320  0.0988\n",
      "     42       37.2320  0.0846\n",
      "     43       37.2320  0.0981\n",
      "     44       37.2320  0.1111\n",
      "     45       37.2320  0.1038\n",
      "     46       37.2320  0.1000\n",
      "     47       37.2320  0.0897\n",
      "     48       37.2320  0.0909\n",
      "     49       37.2320  0.0972\n",
      "     50       37.2320  0.0987\n",
      "     51       37.2320  0.0815\n",
      "     52       37.2320  0.0974\n",
      "     53       37.2320  0.0959\n",
      "     54       37.2320  0.0992\n",
      "     55       37.2320  0.0956\n",
      "     56       37.2320  0.0869\n",
      "     57       37.2320  0.0924\n",
      "     58       37.2320  0.0867\n",
      "     59       37.2320  0.0970\n",
      "     60       37.2320  0.0854\n",
      "     61       37.2320  0.0978\n",
      "     62       37.2320  0.0983\n",
      "     63       37.2320  0.1033\n",
      "     64       37.2320  0.0866\n",
      "     65       37.2320  0.0938\n",
      "     66       37.2320  0.0883\n",
      "     67       37.2320  0.0971\n",
      "     68       37.2320  0.0954\n",
      "     69       37.2320  0.0845\n",
      "     70       37.2320  0.0890\n",
      "     71       37.2320  0.0878\n",
      "     72       37.2320  0.0897\n",
      "     73       37.2320  0.0972\n",
      "     74       37.2320  0.0896\n",
      "     75       37.2320  0.0949\n",
      "     76       37.2320  0.1039\n",
      "     77       37.2320  0.0775\n",
      "     78       37.2320  0.0966\n",
      "     79       37.2320  0.0915\n",
      "     80       37.2320  0.1099\n",
      "     81       37.2320  0.0976\n",
      "     82       37.2320  0.0936\n",
      "     83       37.2320  0.0891\n",
      "     84       37.2320  0.1048\n",
      "     85       37.2320  0.0844\n",
      "     86       37.2320  0.0895\n",
      "     87       37.2320  0.0920\n",
      "     88       37.2320  0.0930\n",
      "     89       37.2320  0.0934\n",
      "     90       37.2320  0.0951\n",
      "     91       37.2320  0.0841\n",
      "     92       37.2320  0.0985\n",
      "     93       37.2320  0.0900\n",
      "     94       37.2320  0.0888\n",
      "     95       37.2320  0.0948\n",
      "     96       37.2320  0.1247\n",
      "     97       37.2320  0.0865\n",
      "     98       37.2320  0.0953\n",
      "     99       37.2320  0.0982\n",
      "    100       37.2320  0.1013\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0781\n",
      "      2       37.2320  0.1004\n",
      "      3       37.2320  0.0955\n",
      "      4       37.2320  0.0973\n",
      "      5       37.2320  0.0886\n",
      "      6       37.2320  0.0921\n",
      "      7       37.2320  0.0925\n",
      "      8       37.2320  0.0940\n",
      "      9       37.2320  0.0934\n",
      "     10       37.2320  0.1037\n",
      "     11       37.2320  0.0945\n",
      "     12       37.2320  0.0991\n",
      "     13       37.2320  0.0942\n",
      "     14       37.2320  0.0989\n",
      "     15       37.2320  0.1029\n",
      "     16       37.2320  0.1012\n",
      "     17       37.2320  0.0982\n",
      "     18       37.2320  0.0855\n",
      "     19       37.2320  0.0962\n",
      "     20       37.2320  0.0835\n",
      "     21       37.2320  0.0898\n",
      "     22       37.2320  0.0887\n",
      "     23       37.2320  0.0970\n",
      "     24       37.2320  0.0873\n",
      "     25       37.2320  0.0890\n",
      "     26       17.4384  0.0859\n",
      "     27        0.6449  0.0917\n",
      "     28        0.6204  0.0899\n",
      "     29        0.6185  0.0901\n",
      "     30        0.6175  0.0876\n",
      "     31        0.6167  0.0943\n",
      "     32        0.6161  0.0919\n",
      "     33        0.6156  0.0902\n",
      "     34        0.6152  0.0985\n",
      "     35        0.6149  0.1221\n",
      "     36        0.6147  0.1058\n",
      "     37        0.6144  0.1011\n",
      "     38        0.6142  0.0989\n",
      "     39        0.6140  0.0975\n",
      "     40        0.6139  0.0931\n",
      "     41        0.6138  0.0989\n",
      "     42        0.6137  0.0929\n",
      "     43        0.6136  0.0959\n",
      "     44        0.6135  0.0948\n",
      "     45        0.6134  0.0962\n",
      "     46        0.6134  0.0892\n",
      "     47        0.6133  0.0993\n",
      "     48        0.6133  0.0847\n",
      "     49        0.6132  0.1009\n",
      "     50        0.6132  0.0962\n",
      "     51        0.6132  0.0994\n",
      "     52        0.6131  0.1214\n",
      "     53        0.6131  0.0909\n",
      "     54        0.6131  0.1029\n",
      "     55        0.6131  0.0967\n",
      "     56        0.6131  0.1052\n",
      "     57        0.6130  0.0963\n",
      "     58        0.6130  0.1033\n",
      "     59        0.6130  0.1036\n",
      "     60        0.6130  0.1017\n",
      "     61        0.6130  0.1025\n",
      "     62        0.6130  0.0893\n",
      "     63        0.6130  0.0887\n",
      "     64        0.6130  0.0972\n",
      "     65        0.6130  0.0864\n",
      "     66        0.6130  0.0921\n",
      "     67        0.6130  0.1022\n",
      "     68        0.6130  0.0904\n",
      "     69        0.6130  0.1234\n",
      "     70        0.6130  0.0997\n",
      "     71        0.6130  0.0877\n",
      "     72        0.6130  0.0965\n",
      "     73        0.6130  0.0879\n",
      "     74        0.6130  0.1039\n",
      "     75        0.6130  0.0953\n",
      "     76        0.6130  0.0929\n",
      "     77        0.6130  0.0946\n",
      "     78        0.6130  0.0845\n",
      "     79        0.6130  0.0836\n",
      "     80        0.6130  0.0975\n",
      "     81        0.6130  0.0906\n",
      "     82        0.6130  0.0850\n",
      "     83        0.6130  0.0920\n",
      "     84        0.6130  0.0851\n",
      "     85        0.6130  0.0864\n",
      "     86        0.6130  0.0972\n",
      "     87        0.6130  0.0897\n",
      "     88        0.6130  0.0972\n",
      "     89        0.6130  0.0916\n",
      "     90        0.6130  0.0944\n",
      "     91        0.6130  0.0908\n",
      "     92        0.6130  0.0937\n",
      "     93        0.6130  0.0975\n",
      "     94        0.6130  0.0973\n",
      "     95        0.6130  0.1074\n",
      "     96        0.6130  0.0994\n",
      "     97        0.6130  0.0899\n",
      "     98        0.6130  0.0894\n",
      "     99        0.6130  0.1165\n",
      "    100        0.6130  0.0904\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(skorch_classifier, X, Y, cv = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=30, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_torch = torch.tensor(np.array(X_test), dtype=torch.float)\n",
    "Y_test_torch = torch.tensor(np.array(Y_test), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.6507e-01],\n",
      "        [1.0000e+00],\n",
      "        [3.0317e-02],\n",
      "        [9.9177e-01],\n",
      "        [4.9362e-05],\n",
      "        [7.3030e-01],\n",
      "        [9.6594e-01],\n",
      "        [9.9950e-01],\n",
      "        [4.7877e-01],\n",
      "        [7.1802e-02]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.forward(X_test_torch)\n",
    "\n",
    "print(predictions[:10][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(predictions > 0.5)\n",
    "\n",
    "print(predictions[:10][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8881118881118881\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
