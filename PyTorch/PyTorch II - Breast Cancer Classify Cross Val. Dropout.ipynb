{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch NN Studies Notebook II - Breast Cancer Classification (Cross Validation + Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "import skorch\n",
    "print(skorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetBinaryClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_path = 'C:/Users/Mafeus/Desktop/Curso PyTorch/Bases/Bases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>706.771388</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>4.304801</td>\n",
       "      <td>4.835984</td>\n",
       "      <td>7.489124</td>\n",
       "      <td>2.366459</td>\n",
       "      <td>16.965766</td>\n",
       "      <td>0.851112</td>\n",
       "      <td>...</td>\n",
       "      <td>315.194921</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>10.633281</td>\n",
       "      <td>25.259112</td>\n",
       "      <td>26.723742</td>\n",
       "      <td>8.745685</td>\n",
       "      <td>30.367174</td>\n",
       "      <td>1.964313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2430.243368</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>21.074558</td>\n",
       "      <td>26.827478</td>\n",
       "      <td>35.618994</td>\n",
       "      <td>16.155145</td>\n",
       "      <td>53.846023</td>\n",
       "      <td>7.103493</td>\n",
       "      <td>...</td>\n",
       "      <td>1655.459336</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>37.236433</td>\n",
       "      <td>96.473015</td>\n",
       "      <td>114.204035</td>\n",
       "      <td>39.465975</td>\n",
       "      <td>90.748044</td>\n",
       "      <td>14.464355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.760000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.210000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086410</td>\n",
       "      <td>0.065260</td>\n",
       "      <td>0.029580</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>...</td>\n",
       "      <td>13.180000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.064990</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.850000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095940</td>\n",
       "      <td>0.094620</td>\n",
       "      <td>0.063870</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>...</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.080060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.680000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.077260</td>\n",
       "      <td>0.203600</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>...</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.092110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9904.000000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9981.000000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>1058.000000</td>\n",
       "      <td>1252.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>544.000000</td>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        radius_mean   texture_mean   perimeter_mean    area_mean  \\\n",
       "count    569.000000     569.000000       569.000000   569.000000   \n",
       "mean     706.771388      19.289649        91.969033   654.889104   \n",
       "std     2430.243368       4.301036        24.298981   351.914129   \n",
       "min        7.760000       9.710000        43.790000   143.500000   \n",
       "25%       12.210000      16.170000        75.170000   420.300000   \n",
       "50%       13.850000      18.840000        86.240000   551.100000   \n",
       "75%       17.680000      21.800000       104.100000   782.700000   \n",
       "max     9904.000000      39.280000       188.500000  2501.000000   \n",
       "\n",
       "        smoothness_mean   compactness_mean   concavity_mean  \\\n",
       "count        569.000000         569.000000       569.000000   \n",
       "mean           4.304801           4.835984         7.489124   \n",
       "std           21.074558          26.827478        35.618994   \n",
       "min            0.052630           0.019380         0.000000   \n",
       "25%            0.086410           0.065260         0.029580   \n",
       "50%            0.095940           0.094620         0.063870   \n",
       "75%            0.106100           0.132500         0.142500   \n",
       "max          123.000000         277.000000       313.000000   \n",
       "\n",
       "       concave_points_mean   symmetry_mean   fractal_dimension_mean  ...  \\\n",
       "count           569.000000      569.000000               569.000000  ...   \n",
       "mean              2.366459       16.965766                 0.851112  ...   \n",
       "std              16.155145       53.846023                 7.103493  ...   \n",
       "min               0.000000        0.116700                 0.049960  ...   \n",
       "25%               0.020310        0.163400                 0.057800  ...   \n",
       "50%               0.033900        0.181400                 0.061660  ...   \n",
       "75%               0.077260        0.203600                 0.066400  ...   \n",
       "max             162.000000      304.000000                78.000000  ...   \n",
       "\n",
       "        radius_worst   texture_worst   perimeter_worst   area_worst  \\\n",
       "count     569.000000      569.000000        569.000000   569.000000   \n",
       "mean      315.194921       25.677223        107.261213   880.583128   \n",
       "std      1655.459336        6.146258         33.602542   569.356993   \n",
       "min         7.930000       12.020000         50.410000   185.200000   \n",
       "25%        13.180000       21.080000         84.110000   515.300000   \n",
       "50%        15.150000       25.410000         97.660000   686.500000   \n",
       "75%        19.850000       29.720000        125.400000  1084.000000   \n",
       "max      9981.000000       49.540000        251.200000  4254.000000   \n",
       "\n",
       "        smoothness_worst   compactness_worst   concavity_worst  \\\n",
       "count         569.000000          569.000000        569.000000   \n",
       "mean           10.633281           25.259112         26.723742   \n",
       "std            37.236433           96.473015        114.204035   \n",
       "min             0.071170            0.027290          0.000000   \n",
       "25%             0.117800            0.150700          0.116800   \n",
       "50%             0.133800            0.227900          0.249200   \n",
       "75%             0.150000            0.384200          0.431600   \n",
       "max           185.000000         1058.000000       1252.000000   \n",
       "\n",
       "        concave_points_worst   symmetry_worst   fractal_dimension_worst  \n",
       "count             569.000000       569.000000                569.000000  \n",
       "mean                8.745685        30.367174                  1.964313  \n",
       "std                39.465975        90.748044                 14.464355  \n",
       "min                 0.000000         0.156500                  0.055040  \n",
       "25%                 0.064990         0.254900                  0.071460  \n",
       "50%                 0.101500         0.288400                  0.080060  \n",
       "75%                 0.170800         0.331800                  0.092110  \n",
       "max               291.000000       544.000000                173.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data  = pd.read_csv(data_base_path + 'entradas_breast.csv')\n",
    "output_data = pd.read_csv(data_base_path + 'saidas_breast.csv')\n",
    "\n",
    "input_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  569.000000\n",
       "mean     0.627417\n",
       "std      0.483918\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(output_data['0']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x191269e0060>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(input_data, dtype=np.float32)\n",
    "Y = np.array(output_data, dtype=np.float32).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Model Build Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 30 -> 16 -> 16 -> 1\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight)\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "skorch_classifier = NeuralNetBinaryClassifier(module=torch_classifier, criterion=torch.nn.BCELoss,\n",
    "                                              optimizer=torch.optim.Adam, lr=0.001,\n",
    "                                              optimizer__weight_decay=0.0001, max_epochs=100, \n",
    "                                              batch_size=10, train_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:665: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_folds = np.zeros(n_samples, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1820  2.7376\n",
      "      2       37.1820  0.0856\n",
      "      3       37.1820  0.0878\n",
      "      4       37.1820  0.0931\n",
      "      5       37.1820  0.0944\n",
      "      6       37.1820  0.0944\n",
      "      7       37.1820  0.0983\n",
      "      8       37.1820  0.0983\n",
      "      9       37.1820  0.0957\n",
      "     10       37.1820  0.0962\n",
      "     11       37.1820  0.0996\n",
      "     12       37.1820  0.0899\n",
      "     13       37.1820  0.0861\n",
      "     14       37.1820  0.0886\n",
      "     15       37.1820  0.1206\n",
      "     16       37.1820  0.0966\n",
      "     17       37.1820  0.1160\n",
      "     18       37.1820  0.1017\n",
      "     19       37.1820  0.1007\n",
      "     20       37.1820  0.1016\n",
      "     21       37.1820  0.1060\n",
      "     22       37.1820  0.0995\n",
      "     23       37.1820  0.1023\n",
      "     24       37.1820  0.0993\n",
      "     25       13.6727  0.1031\n",
      "     26        0.5540  0.1087\n",
      "     27        0.5328  0.1120\n",
      "     28        0.5156  0.1147\n",
      "     29        0.4943  0.1227\n",
      "     30        0.4848  0.0915\n",
      "     31        0.4790  0.1171\n",
      "     32        0.4740  0.1005\n",
      "     33        0.4691  0.1183\n",
      "     34        0.4642  0.1419\n",
      "     35        0.4597  0.1536\n",
      "     36        0.4544  0.1132\n",
      "     37        0.4487  0.1182\n",
      "     38        0.4430  0.1337\n",
      "     39        0.4379  0.1094\n",
      "     40        0.4329  0.0926\n",
      "     41        0.4265  0.1089\n",
      "     42        0.4206  0.1357\n",
      "     43        0.4143  0.0990\n",
      "     44        0.4070  0.1012\n",
      "     45        0.3962  0.1516\n",
      "     46        0.3859  0.1019\n",
      "     47        0.3759  0.1505\n",
      "     48        0.3651  0.0952\n",
      "     49        0.3545  0.1018\n",
      "     50        0.3432  0.0885\n",
      "     51        0.3335  0.1004\n",
      "     52        0.3212  0.1185\n",
      "     53        0.3167  0.0999\n",
      "     54        0.3044  0.1107\n",
      "     55        0.3013  0.0990\n",
      "     56        0.2891  0.1201\n",
      "     57        0.2826  0.1097\n",
      "     58        0.2738  0.1057\n",
      "     59        0.2684  0.0998\n",
      "     60        0.2631  0.0963\n",
      "     61        0.2546  0.1068\n",
      "     62        0.2474  0.1390\n",
      "     63        0.2429  0.1071\n",
      "     64        0.2574  0.1335\n",
      "     65        0.2336  0.1257\n",
      "     66        0.2266  0.0991\n",
      "     67        0.2207  0.1050\n",
      "     68        0.2243  0.1177\n",
      "     69        0.2130  0.1362\n",
      "     70        0.2108  0.1177\n",
      "     71        0.2134  0.1077\n",
      "     72        0.2055  0.0975\n",
      "     73        0.2008  0.1225\n",
      "     74        0.2015  0.0971\n",
      "     75        0.1988  0.1010\n",
      "     76        0.1958  0.1047\n",
      "     77        0.1954  0.1031\n",
      "     78        0.1922  0.1005\n",
      "     79        0.1864  0.1361\n",
      "     80        0.1879  0.0976\n",
      "     81        0.1761  0.1105\n",
      "     82        0.1734  0.0925\n",
      "     83        0.1766  0.1104\n",
      "     84        0.1712  0.1254\n",
      "     85        0.1601  0.1042\n",
      "     86        0.1748  0.1001\n",
      "     87        0.1775  0.1030\n",
      "     88        0.1628  0.1022\n",
      "     89        0.1553  0.1019\n",
      "     90        0.1536  0.1043\n",
      "     91        0.1551  0.1199\n",
      "     92        0.1682  0.1032\n",
      "     93        0.1753  0.1201\n",
      "     94        0.1628  0.1195\n",
      "     95        0.1647  0.0872\n",
      "     96        0.1570  0.1035\n",
      "     97        0.1488  0.1210\n",
      "     98        0.1500  0.1158\n",
      "     99        0.1458  0.1034\n",
      "    100        0.1401  0.1165\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1820  0.1094\n",
      "      2       37.1820  0.1065\n",
      "      3       37.1820  0.1247\n",
      "      4       37.1820  0.1187\n",
      "      5       37.1820  0.0997\n",
      "      6       37.1820  0.0995\n",
      "      7       37.1820  0.1151\n",
      "      8       37.1820  0.1035\n",
      "      9       37.1820  0.1297\n",
      "     10       37.1820  0.1037\n",
      "     11       37.1820  0.1009\n",
      "     12       37.1820  0.1093\n",
      "     13       37.1820  0.1077\n",
      "     14       37.1820  0.1058\n",
      "     15       37.1820  0.1001\n",
      "     16       37.1820  0.0860\n",
      "     17       37.1820  0.1078\n",
      "     18       37.1820  0.1283\n",
      "     19       37.1820  0.1177\n",
      "     20       37.1820  0.1127\n",
      "     21       37.1820  0.1137\n",
      "     22       37.1820  0.1217\n",
      "     23       37.1820  0.1117\n",
      "     24       37.1820  0.1349\n",
      "     25       18.2752  0.1037\n",
      "     26        0.5688  0.1191\n",
      "     27        0.5527  0.1349\n",
      "     28        0.5499  0.1022\n",
      "     29        0.5490  0.1017\n",
      "     30        0.5457  0.1216\n",
      "     31        0.5508  0.1105\n",
      "     32        0.5391  0.0902\n",
      "     33        0.5351  0.1116\n",
      "     34        0.5271  0.1261\n",
      "     35        0.5229  0.1184\n",
      "     36        0.5015  0.1391\n",
      "     37        0.4818  0.1137\n",
      "     38        0.4613  0.1127\n",
      "     39        0.4393  0.1267\n",
      "     40        0.4157  0.1685\n",
      "     41        0.3978  0.1132\n",
      "     42        0.3834  0.1333\n",
      "     43        0.3687  0.1461\n",
      "     44        0.3535  0.0914\n",
      "     45        0.3389  0.1028\n",
      "     46        0.3239  0.1195\n",
      "     47        0.3073  0.1182\n",
      "     48        0.2803  0.1172\n",
      "     49        0.2494  0.1176\n",
      "     50        0.2273  0.1177\n",
      "     51        0.2131  0.1179\n",
      "     52        0.2028  0.1172\n",
      "     53        0.1954  0.1031\n",
      "     54        0.1906  0.1176\n",
      "     55        0.1886  0.1310\n",
      "     56        0.1844  0.1064\n",
      "     57        0.1830  0.1172\n",
      "     58        0.1767  0.1194\n",
      "     59        0.1726  0.1613\n",
      "     60        0.1689  0.1009\n",
      "     61        0.1665  0.1215\n",
      "     62        0.1630  0.1005\n",
      "     63        0.1624  0.0987\n",
      "     64        0.1561  0.1227\n",
      "     65        0.1544  0.1157\n",
      "     66        0.1548  0.1107\n",
      "     67        0.1478  0.0942\n",
      "     68        0.1472  0.1346\n",
      "     69        0.1457  0.1132\n",
      "     70        0.1400  0.1218\n",
      "     71        0.1388  0.1301\n",
      "     72        0.1369  0.1217\n",
      "     73        0.1353  0.1147\n",
      "     74        0.1347  0.0989\n",
      "     75        0.1317  0.1127\n",
      "     76        0.1308  0.1245\n",
      "     77        0.1281  0.1093\n",
      "     78        0.1280  0.1087\n",
      "     79        0.1269  0.1046\n",
      "     80        0.1262  0.1006\n",
      "     81        0.1257  0.1066\n",
      "     82        0.1233  0.1037\n",
      "     83        0.1234  0.1158\n",
      "     84        0.1215  0.1065\n",
      "     85        0.1213  0.1008\n",
      "     86        0.1191  0.1032\n",
      "     87        0.1181  0.1044\n",
      "     88        0.1185  0.1211\n",
      "     89        0.1192  0.1031\n",
      "     90        0.1179  0.1031\n",
      "     91        0.1130  0.1114\n",
      "     92        0.1114  0.1023\n",
      "     93        0.1122  0.1187\n",
      "     94        0.1080  0.1175\n",
      "     95        0.1112  0.1187\n",
      "     96        0.1066  0.1183\n",
      "     97        0.1103  0.1046\n",
      "     98        0.1084  0.1034\n",
      "     99        0.1095  0.1025\n",
      "    100        0.1081  0.1038\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0990\n",
      "      2       37.3047  0.1138\n",
      "      3       37.3047  0.1133\n",
      "      4       37.3047  0.1190\n",
      "      5       37.3047  0.1047\n",
      "      6       37.3047  0.0998\n",
      "      7       37.3047  0.1145\n",
      "      8       37.3047  0.1013\n",
      "      9       37.3047  0.1018\n",
      "     10       37.3047  0.1017\n",
      "     11       37.3047  0.1006\n",
      "     12       37.3047  0.0990\n",
      "     13       37.3047  0.1037\n",
      "     14       37.3047  0.1013\n",
      "     15       37.3047  0.0993\n",
      "     16       37.3047  0.1165\n",
      "     17       37.3047  0.1030\n",
      "     18       37.3047  0.0991\n",
      "     19       37.3047  0.1035\n",
      "     20       37.3047  0.1008\n",
      "     21       37.3047  0.1014\n",
      "     22       37.3047  0.0986\n",
      "     23       37.3047  0.1184\n",
      "     24       37.3047  0.1016\n",
      "     25       37.3047  0.1155\n",
      "     26       12.0554  0.1027\n",
      "     27        0.6218  0.1188\n",
      "     28        0.5598  0.1166\n",
      "     29        0.5421  0.1173\n",
      "     30        0.5217  0.1322\n",
      "     31        0.5103  0.1190\n",
      "     32        0.5066  0.1180\n",
      "     33        0.4987  0.1168\n",
      "     34        0.4896  0.1161\n",
      "     35        0.4823  0.1020\n",
      "     36        0.4724  0.1115\n",
      "     37        0.4708  0.1183\n",
      "     38        0.4657  0.1054\n",
      "     39        0.4598  0.1227\n",
      "     40        0.4518  0.0838\n",
      "     41        0.4435  0.1046\n",
      "     42        0.4372  0.1313\n",
      "     43        0.4256  0.1426\n",
      "     44        0.4203  0.1157\n",
      "     45        0.4111  0.1190\n",
      "     46        0.3965  0.1165\n",
      "     47        0.4017  0.1008\n",
      "     48        0.3832  0.1003\n",
      "     49        0.3736  0.1168\n",
      "     50        0.3779  0.1033\n",
      "     51        0.3566  0.1127\n",
      "     52        0.3550  0.1020\n",
      "     53        0.3428  0.1049\n",
      "     54        0.3252  0.1013\n",
      "     55        0.3170  0.1025\n",
      "     56        0.3042  0.1198\n",
      "     57        0.2979  0.1006\n",
      "     58        0.2971  0.1026\n",
      "     59        0.2881  0.1015\n",
      "     60        0.2871  0.1036\n",
      "     61        0.2770  0.1032\n",
      "     62        0.2702  0.1376\n",
      "     63        0.2754  0.1145\n",
      "     64        0.2697  0.1040\n",
      "     65        0.2695  0.1002\n",
      "     66        0.2597  0.1036\n",
      "     67        0.2585  0.1425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     68        0.2540  0.1194\n",
      "     69        0.2561  0.1032\n",
      "     70        0.2462  0.0874\n",
      "     71        0.2495  0.1025\n",
      "     72        0.2406  0.1343\n",
      "     73        0.2395  0.0965\n",
      "     74        0.2381  0.1016\n",
      "     75        0.2384  0.1192\n",
      "     76        0.2341  0.0877\n",
      "     77        0.2350  0.1188\n",
      "     78        0.2299  0.1032\n",
      "     79        0.2284  0.1008\n",
      "     80        0.2224  0.1038\n",
      "     81        0.2209  0.1025\n",
      "     82        0.2182  0.1028\n",
      "     83        0.2167  0.1032\n",
      "     84        0.2114  0.1026\n",
      "     85        0.2118  0.0901\n",
      "     86        0.2079  0.1041\n",
      "     87        0.2095  0.1042\n",
      "     88        0.2069  0.0869\n",
      "     89        0.2078  0.1169\n",
      "     90        0.2030  0.1036\n",
      "     91        0.2033  0.1043\n",
      "     92        0.1995  0.1036\n",
      "     93        0.1984  0.1022\n",
      "     94        0.1988  0.1039\n",
      "     95        0.1927  0.1037\n",
      "     96        0.1987  0.1036\n",
      "     97        0.1910  0.1027\n",
      "     98        0.1879  0.1023\n",
      "     99        0.1882  0.1043\n",
      "    100        0.1836  0.1043\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0842\n",
      "      2       37.3047  0.1006\n",
      "      3       37.3047  0.1143\n",
      "      4       37.3047  0.0861\n",
      "      5       37.3047  0.1092\n",
      "      6       37.3047  0.1077\n",
      "      7       37.3047  0.1161\n",
      "      8       37.3047  0.0991\n",
      "      9       37.3047  0.1463\n",
      "     10       37.3047  0.1071\n",
      "     11       37.3047  0.0992\n",
      "     12       37.3047  0.1324\n",
      "     13       37.3047  0.1032\n",
      "     14       37.3047  0.1002\n",
      "     15       37.3047  0.1007\n",
      "     16       37.3047  0.1013\n",
      "     17       37.3047  0.1006\n",
      "     18       37.3047  0.0995\n",
      "     19       37.3047  0.1011\n",
      "     20       37.3047  0.1003\n",
      "     21       37.3047  0.1409\n",
      "     22       37.3047  0.1316\n",
      "     23       37.3047  0.0922\n",
      "     24       37.3047  0.1010\n",
      "     25       37.3047  0.0987\n",
      "     26       37.3047  0.1297\n",
      "     27       37.3047  0.1287\n",
      "     28       37.3047  0.0965\n",
      "     29       37.3047  0.1170\n",
      "     30       37.3047  0.1224\n",
      "     31       37.3047  0.1247\n",
      "     32       37.3047  0.1177\n",
      "     33       37.3047  0.0989\n",
      "     34       37.3047  0.1027\n",
      "     35       37.3047  0.1339\n",
      "     36       37.3047  0.1037\n",
      "     37       37.3047  0.0988\n",
      "     38       37.3047  0.1023\n",
      "     39       37.3047  0.1155\n",
      "     40       37.3047  0.1277\n",
      "     41       37.3047  0.0961\n",
      "     42       37.3047  0.1390\n",
      "     43       37.3047  0.1219\n",
      "     44       37.3047  0.1259\n",
      "     45       37.3047  0.1211\n",
      "     46       37.3047  0.1168\n",
      "     47       37.3047  0.1346\n",
      "     48       37.3047  0.0970\n",
      "     49       37.3047  0.1279\n",
      "     50       37.3047  0.1168\n",
      "     51       37.3047  0.0926\n",
      "     52       37.3047  0.1013\n",
      "     53       37.3047  0.1035\n",
      "     54       37.3047  0.1010\n",
      "     55       37.3047  0.0876\n",
      "     56       37.3047  0.1107\n",
      "     57       37.3047  0.1027\n",
      "     58       37.3047  0.1047\n",
      "     59       37.3047  0.1003\n",
      "     60       37.3047  0.1031\n",
      "     61       37.3047  0.0959\n",
      "     62       37.3047  0.0982\n",
      "     63       37.3047  0.1172\n",
      "     64       37.3047  0.1621\n",
      "     65       37.3047  0.1341\n",
      "     66       37.3047  0.1870\n",
      "     67       37.3047  0.1523\n",
      "     68       37.3047  0.1122\n",
      "     69       37.3047  0.1114\n",
      "     70       37.3047  0.1023\n",
      "     71       37.3047  0.1023\n",
      "     72       37.3047  0.1017\n",
      "     73       37.3047  0.1029\n",
      "     74       37.3047  0.1012\n",
      "     75       37.3047  0.1158\n",
      "     76       37.3047  0.1024\n",
      "     77       37.3047  0.1014\n",
      "     78       37.3047  0.1006\n",
      "     79       37.3047  0.1043\n",
      "     80       37.3047  0.1032\n",
      "     81       37.3047  0.1174\n",
      "     82       37.3047  0.0872\n",
      "     83       37.3047  0.1038\n",
      "     84       37.3047  0.1008\n",
      "     85       37.3047  0.1038\n",
      "     86       37.3047  0.1027\n",
      "     87       37.3047  0.1123\n",
      "     88       37.3047  0.1057\n",
      "     89       37.3047  0.0989\n",
      "     90       37.3047  0.1021\n",
      "     91       37.3047  0.1184\n",
      "     92       37.3047  0.1107\n",
      "     93       37.3047  0.1160\n",
      "     94       37.3047  0.1190\n",
      "     95       37.3047  0.1385\n",
      "     96       37.3047  0.0998\n",
      "     97       37.3047  0.1054\n",
      "     98       37.3047  0.1027\n",
      "     99       37.3047  0.1034\n",
      "    100       37.3047  0.1021\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.1247\n",
      "      2       37.3047  0.1028\n",
      "      3       37.3047  0.1016\n",
      "      4       37.3047  0.1012\n",
      "      5       37.3047  0.1154\n",
      "      6       37.3047  0.1003\n",
      "      7       37.3047  0.1004\n",
      "      8       37.3047  0.1010\n",
      "      9       37.3047  0.1480\n",
      "     10       37.3047  0.1031\n",
      "     11       37.3047  0.1110\n",
      "     12       37.3047  0.1023\n",
      "     13       37.3047  0.1175\n",
      "     14       37.3047  0.1012\n",
      "     15       37.3047  0.1011\n",
      "     16       37.3047  0.1396\n",
      "     17       37.3047  0.1182\n",
      "     18       37.3047  0.1006\n",
      "     19       37.3047  0.1014\n",
      "     20       37.3047  0.1012\n",
      "     21       37.3047  0.1005\n",
      "     22       37.3047  0.1020\n",
      "     23       37.3047  0.0984\n",
      "     24       37.3047  0.1032\n",
      "     25       37.3047  0.1167\n",
      "     26       37.3047  0.0999\n",
      "     27       15.4257  0.1149\n",
      "     28        0.5390  0.1416\n",
      "     29        0.5115  0.0980\n",
      "     30        0.4965  0.1196\n",
      "     31        0.4822  0.1020\n",
      "     32        0.4651  0.1010\n",
      "     33        0.4528  0.1247\n",
      "     34        0.4388  0.1097\n",
      "     35        0.4185  0.0956\n",
      "     36        0.4334  0.1186\n",
      "     37        0.4169  0.1012\n",
      "     38        0.4174  0.1009\n",
      "     39        0.3996  0.1165\n",
      "     40        0.3995  0.1127\n",
      "     41        0.3831  0.1076\n",
      "     42        0.3797  0.1237\n",
      "     43        0.3698  0.1067\n",
      "     44        0.3597  0.1113\n",
      "     45        0.3801  0.1184\n",
      "     46        0.3474  0.1011\n",
      "     47        0.3430  0.1168\n",
      "     48        0.3401  0.1021\n",
      "     49        0.3382  0.1000\n",
      "     50        0.3214  0.1201\n",
      "     51        0.3104  0.1245\n",
      "     52        0.3056  0.1013\n",
      "     53        0.2987  0.1197\n",
      "     54        0.2946  0.1117\n",
      "     55        0.2853  0.0912\n",
      "     56        0.2775  0.1166\n",
      "     57        0.2720  0.1055\n",
      "     58        0.2716  0.1260\n",
      "     59        0.2610  0.1310\n",
      "     60        0.2551  0.1110\n",
      "     61        0.2510  0.1180\n",
      "     62        0.2413  0.1174\n",
      "     63        0.2371  0.1409\n",
      "     64        0.2350  0.1086\n",
      "     65        0.2336  0.1110\n",
      "     66        0.2307  0.1045\n",
      "     67        0.2123  0.1136\n",
      "     68        0.2131  0.1057\n",
      "     69        0.2109  0.1023\n",
      "     70        0.2141  0.1151\n",
      "     71        0.2053  0.1048\n",
      "     72        0.2084  0.1188\n",
      "     73        0.2049  0.1017\n",
      "     74        0.2061  0.1212\n",
      "     75        0.1985  0.1058\n",
      "     76        0.1930  0.1143\n",
      "     77        0.1920  0.1025\n",
      "     78        0.1909  0.1041\n",
      "     79        0.1878  0.1033\n",
      "     80        0.1925  0.1020\n",
      "     81        0.1841  0.1032\n",
      "     82        0.1774  0.1057\n",
      "     83        0.1817  0.1027\n",
      "     84        0.1755  0.1175\n",
      "     85        0.1772  0.1214\n",
      "     86        0.1758  0.1023\n",
      "     87        0.1710  0.1177\n",
      "     88        0.1733  0.1035\n",
      "     89        0.1695  0.1055\n",
      "     90        0.1679  0.1194\n",
      "     91        0.1747  0.1037\n",
      "     92        0.1604  0.1103\n",
      "     93        0.1649  0.1230\n",
      "     94        0.1597  0.1137\n",
      "     95        0.1634  0.1183\n",
      "     96        0.1633  0.1107\n",
      "     97        0.1623  0.1149\n",
      "     98        0.1552  0.1260\n",
      "     99        0.1584  0.1023\n",
      "    100        0.1532  0.1269\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0906\n",
      "      2       37.3047  0.1247\n",
      "      3       37.3047  0.1246\n",
      "      4       37.3047  0.1302\n",
      "      5       37.3047  0.1309\n",
      "      6       37.3047  0.2012\n",
      "      7       37.3047  0.1091\n",
      "      8       37.3047  0.0851\n",
      "      9       37.3047  0.0990\n",
      "     10       37.3047  0.1248\n",
      "     11       37.3047  0.1197\n",
      "     12       37.3047  0.1077\n",
      "     13       37.3047  0.0853\n",
      "     14       37.3047  0.1027\n",
      "     15       37.3047  0.0923\n",
      "     16       37.3047  0.0896\n",
      "     17       37.3047  0.1624\n",
      "     18       37.3047  0.1506\n",
      "     19       37.3047  0.1346\n",
      "     20       37.3047  0.1287\n",
      "     21       37.3047  0.0847\n",
      "     22       37.3047  0.0937\n",
      "     23       35.1446  0.1077\n",
      "     24        0.5570  0.0985\n",
      "     25        0.5403  0.1010\n",
      "     26        0.4892  0.1015\n",
      "     27        0.4658  0.1113\n",
      "     28        0.4427  0.1326\n",
      "     29        0.4252  0.0928\n",
      "     30        0.4094  0.1040\n",
      "     31        0.4011  0.0921\n",
      "     32        0.3859  0.1166\n",
      "     33        0.3755  0.0882\n",
      "     34        0.3605  0.1310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35        0.3451  0.1087\n",
      "     36        0.3408  0.0968\n",
      "     37        0.3301  0.0995\n",
      "     38        0.3217  0.1279\n",
      "     39        0.3137  0.1289\n",
      "     40        0.3026  0.0860\n",
      "     41        0.2972  0.1181\n",
      "     42        0.2913  0.1297\n",
      "     43        0.2876  0.0927\n",
      "     44        0.2823  0.1266\n",
      "     45        0.2755  0.1008\n",
      "     46        0.2685  0.1103\n",
      "     47        0.2630  0.0912\n",
      "     48        0.2573  0.1109\n",
      "     49        0.2530  0.0909\n",
      "     50        0.2437  0.0936\n",
      "     51        0.2378  0.0934\n",
      "     52        0.2276  0.1133\n",
      "     53        0.2234  0.1043\n",
      "     54        0.2159  0.1033\n",
      "     55        0.2126  0.0890\n",
      "     56        0.2068  0.1007\n",
      "     57        0.2055  0.1217\n",
      "     58        0.1999  0.0951\n",
      "     59        0.2033  0.0954\n",
      "     60        0.1994  0.0850\n",
      "     61        0.1957  0.1002\n",
      "     62        0.1930  0.1177\n",
      "     63        0.1903  0.1148\n",
      "     64        0.1919  0.1164\n",
      "     65        0.1890  0.0817\n",
      "     66        0.1890  0.1456\n",
      "     67        0.1865  0.0858\n",
      "     68        0.1853  0.0885\n",
      "     69        0.1735  0.1004\n",
      "     70        0.1793  0.0909\n",
      "     71        0.1735  0.0926\n",
      "     72        0.1731  0.0950\n",
      "     73        0.1691  0.1105\n",
      "     74        0.1663  0.0900\n",
      "     75        0.1629  0.0950\n",
      "     76        0.1592  0.0950\n",
      "     77        0.1612  0.0980\n",
      "     78        0.1553  0.0958\n",
      "     79        0.1576  0.0991\n",
      "     80        0.1577  0.1112\n",
      "     81        0.1587  0.0992\n",
      "     82        0.1568  0.0927\n",
      "     83        0.1585  0.0860\n",
      "     84        0.1594  0.0987\n",
      "     85        0.1542  0.0979\n",
      "     86        0.1557  0.0934\n",
      "     87        0.1552  0.0994\n",
      "     88        0.1504  0.0990\n",
      "     89        0.1491  0.0846\n",
      "     90        0.1555  0.1047\n",
      "     91        0.1489  0.1177\n",
      "     92        0.1462  0.0905\n",
      "     93        0.1432  0.0993\n",
      "     94        0.1481  0.1267\n",
      "     95        0.1437  0.1067\n",
      "     96        0.1423  0.1191\n",
      "     97        0.1496  0.1022\n",
      "     98        0.1432  0.0873\n",
      "     99        0.1444  0.0997\n",
      "    100        0.1451  0.0963\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0781\n",
      "      2       37.3047  0.0963\n",
      "      3       37.3047  0.0949\n",
      "      4       37.3047  0.0869\n",
      "      5       37.3047  0.1098\n",
      "      6       37.3047  0.1187\n",
      "      7       37.3047  0.0884\n",
      "      8       37.3047  0.0989\n",
      "      9       37.3047  0.1256\n",
      "     10       37.3047  0.0792\n",
      "     11       37.3047  0.0842\n",
      "     12       37.3047  0.0868\n",
      "     13       37.3047  0.0930\n",
      "     14       37.3047  0.0973\n",
      "     15       37.3047  0.0836\n",
      "     16       37.3047  0.0926\n",
      "     17       37.3047  0.1130\n",
      "     18       37.3047  0.1227\n",
      "     19       37.3047  0.0894\n",
      "     20       37.3047  0.0850\n",
      "     21       37.3047  0.1133\n",
      "     22       37.3047  0.0957\n",
      "     23       37.3047  0.0973\n",
      "     24       37.3047  0.1089\n",
      "     25       16.2499  0.0924\n",
      "     26        0.5846  0.1035\n",
      "     27        0.5754  0.1142\n",
      "     28        0.5617  0.1016\n",
      "     29        0.5579  0.1050\n",
      "     30        0.5474  0.1270\n",
      "     31        0.4928  0.1050\n",
      "     32        0.4689  0.1136\n",
      "     33        0.4484  0.1200\n",
      "     34        0.4265  0.1159\n",
      "     35        0.4068  0.1500\n",
      "     36        0.3939  0.1146\n",
      "     37        0.3687  0.1021\n",
      "     38        0.3532  0.1033\n",
      "     39        0.3287  0.1187\n",
      "     40        0.3233  0.1015\n",
      "     41        0.3063  0.1007\n",
      "     42        0.3025  0.1117\n",
      "     43        0.2931  0.1087\n",
      "     44        0.2888  0.1089\n",
      "     45        0.2871  0.1041\n",
      "     46        0.2904  0.1257\n",
      "     47        0.2862  0.1247\n",
      "     48        0.2845  0.1087\n",
      "     49        0.2839  0.0995\n",
      "     50        0.2819  0.1139\n",
      "     51        0.2807  0.1127\n",
      "     52        0.2810  0.1079\n",
      "     53        0.2764  0.1102\n",
      "     54        0.2773  0.0959\n",
      "     55        0.2768  0.1188\n",
      "     56        0.2702  0.0986\n",
      "     57        0.2594  0.1287\n",
      "     58        0.2551  0.0927\n",
      "     59        0.2571  0.1202\n",
      "     60        0.2504  0.1007\n",
      "     61        0.2424  0.1357\n",
      "     62        0.2408  0.1103\n",
      "     63        0.2354  0.1177\n",
      "     64        0.2337  0.1223\n",
      "     65        0.2266  0.0914\n",
      "     66        0.2274  0.1346\n",
      "     67        0.2218  0.1217\n",
      "     68        0.2240  0.1178\n",
      "     69        0.2147  0.1143\n",
      "     70        0.2121  0.1118\n",
      "     71        0.2072  0.1135\n",
      "     72        0.2065  0.1078\n",
      "     73        0.2076  0.1256\n",
      "     74        0.2061  0.1167\n",
      "     75        0.1971  0.1336\n",
      "     76        0.1960  0.1316\n",
      "     77        0.2005  0.1715\n",
      "     78        0.1976  0.1009\n",
      "     79        0.1981  0.1189\n",
      "     80        0.1963  0.1267\n",
      "     81        0.1961  0.0935\n",
      "     82        0.2015  0.1102\n",
      "     83        0.1968  0.1077\n",
      "     84        0.2009  0.0919\n",
      "     85        0.1930  0.0928\n",
      "     86        0.2012  0.0921\n",
      "     87        0.1968  0.1446\n",
      "     88        0.1931  0.1200\n",
      "     89        0.1900  0.0884\n",
      "     90        0.1894  0.0923\n",
      "     91        0.1939  0.0931\n",
      "     92        0.1977  0.0903\n",
      "     93        0.1855  0.0971\n",
      "     94        0.1939  0.0917\n",
      "     95        0.1828  0.0864\n",
      "     96        0.1803  0.0989\n",
      "     97        0.1860  0.1080\n",
      "     98        0.1821  0.0986\n",
      "     99        0.1972  0.0867\n",
      "    100        0.1821  0.1017\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0781\n",
      "      2       37.2320  0.0977\n",
      "      3       37.2320  0.0868\n",
      "      4       37.2320  0.0970\n",
      "      5       37.2320  0.0857\n",
      "      6       37.2320  0.0847\n",
      "      7       37.2320  0.0781\n",
      "      8       37.2320  0.0882\n",
      "      9       37.2320  0.0871\n",
      "     10       37.2320  0.0968\n",
      "     11       37.2320  0.0877\n",
      "     12       37.2320  0.0915\n",
      "     13       37.2320  0.0830\n",
      "     14       37.2320  0.0945\n",
      "     15       37.2320  0.0968\n",
      "     16       37.2320  0.0865\n",
      "     17       37.2320  0.0867\n",
      "     18       37.2320  0.0890\n",
      "     19       37.2320  0.0924\n",
      "     20       37.2320  0.0993\n",
      "     21       37.2320  0.0977\n",
      "     22       18.0027  0.1225\n",
      "     23        0.6662  0.0981\n",
      "     24        0.5660  0.0978\n",
      "     25        0.5221  0.0956\n",
      "     26        0.5008  0.0984\n",
      "     27        0.4837  0.0951\n",
      "     28        0.4692  0.0940\n",
      "     29        0.4494  0.1256\n",
      "     30        0.4316  0.0990\n",
      "     31        0.4069  0.1006\n",
      "     32        0.3906  0.0974\n",
      "     33        0.3775  0.1002\n",
      "     34        0.3636  0.1363\n",
      "     35        0.3564  0.1021\n",
      "     36        0.3474  0.0933\n",
      "     37        0.3413  0.1130\n",
      "     38        0.3329  0.0986\n",
      "     39        0.3263  0.1289\n",
      "     40        0.3165  0.1033\n",
      "     41        0.3126  0.1004\n",
      "     42        0.3075  0.0963\n",
      "     43        0.3058  0.0928\n",
      "     44        0.3043  0.0897\n",
      "     45        0.2976  0.1037\n",
      "     46        0.2922  0.0885\n",
      "     47        0.2930  0.0883\n",
      "     48        0.2897  0.0937\n",
      "     49        0.2850  0.1029\n",
      "     50        0.2834  0.1043\n",
      "     51        0.2781  0.0921\n",
      "     52        0.2727  0.1027\n",
      "     53        0.2704  0.0983\n",
      "     54        0.2695  0.0887\n",
      "     55        0.2648  0.1153\n",
      "     56        0.2617  0.1086\n",
      "     57        0.2592  0.0976\n",
      "     58        0.2588  0.0901\n",
      "     59        0.2554  0.0956\n",
      "     60        0.2491  0.0997\n",
      "     61        0.2485  0.0909\n",
      "     62        0.2475  0.0998\n",
      "     63        0.2460  0.1085\n",
      "     64        0.2458  0.0910\n",
      "     65        0.2424  0.0922\n",
      "     66        0.2395  0.1329\n",
      "     67        0.2358  0.0905\n",
      "     68        0.2369  0.0990\n",
      "     69        0.2340  0.0973\n",
      "     70        0.2202  0.0997\n",
      "     71        0.2334  0.1185\n",
      "     72        0.2287  0.0848\n",
      "     73        0.2217  0.1147\n",
      "     74        0.2197  0.0955\n",
      "     75        0.2108  0.0981\n",
      "     76        0.2148  0.0970\n",
      "     77        0.2126  0.0968\n",
      "     78        0.2167  0.1057\n",
      "     79        0.2074  0.1049\n",
      "     80        0.1973  0.1043\n",
      "     81        0.1991  0.0905\n",
      "     82        0.1978  0.0886\n",
      "     83        0.1924  0.0890\n",
      "     84        0.1883  0.0896\n",
      "     85        0.1865  0.1000\n",
      "     86        0.1868  0.1034\n",
      "     87        0.1867  0.1038\n",
      "     88        0.1846  0.1276\n",
      "     89        0.1829  0.0901\n",
      "     90        0.1780  0.1019\n",
      "     91        0.1828  0.0898\n",
      "     92        0.1838  0.0901\n",
      "     93        0.1760  0.0941\n",
      "     94        0.1726  0.1097\n",
      "     95        0.1703  0.1013\n",
      "     96        0.1719  0.1060\n",
      "     97        0.1688  0.0906\n",
      "     98        0.1681  0.1005\n",
      "     99        0.1600  0.0977\n",
      "    100        0.1572  0.0928\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2       37.2320  0.1023\n",
      "      3       37.2320  0.0944\n",
      "      4       37.2320  0.0893\n",
      "      5       37.2320  0.1056\n",
      "      6       37.2320  0.0922\n",
      "      7       37.2320  0.0984\n",
      "      8       37.2320  0.0925\n",
      "      9       37.2320  0.0934\n",
      "     10       37.2320  0.0910\n",
      "     11       37.2320  0.0844\n",
      "     12       37.2320  0.1095\n",
      "     13       37.2320  0.0982\n",
      "     14       37.2320  0.0967\n",
      "     15       37.2320  0.0957\n",
      "     16       37.2320  0.0961\n",
      "     17       37.2320  0.0954\n",
      "     18       37.2320  0.0991\n",
      "     19       37.2320  0.0964\n",
      "     20       37.2320  0.0825\n",
      "     21       37.2320  0.0874\n",
      "     22       37.2320  0.0922\n",
      "     23       37.2320  0.0846\n",
      "     24       37.2320  0.0918\n",
      "     25       37.2320  0.0835\n",
      "     26       37.2320  0.1080\n",
      "     27       37.2320  0.0955\n",
      "     28       37.2320  0.0864\n",
      "     29       37.2320  0.0849\n",
      "     30       37.2320  0.0873\n",
      "     31       37.2320  0.1068\n",
      "     32       37.2320  0.0996\n",
      "     33       37.2320  0.0938\n",
      "     34       37.2320  0.0912\n",
      "     35       37.2320  0.0903\n",
      "     36       37.2320  0.0855\n",
      "     37       37.2320  0.0868\n",
      "     38       37.2320  0.0915\n",
      "     39       37.2320  0.0971\n",
      "     40       37.2320  0.0944\n",
      "     41       37.2320  0.0988\n",
      "     42       37.2320  0.0846\n",
      "     43       37.2320  0.0981\n",
      "     44       37.2320  0.1111\n",
      "     45       37.2320  0.1038\n",
      "     46       37.2320  0.1000\n",
      "     47       37.2320  0.0897\n",
      "     48       37.2320  0.0909\n",
      "     49       37.2320  0.0972\n",
      "     50       37.2320  0.0987\n",
      "     51       37.2320  0.0815\n",
      "     52       37.2320  0.0974\n",
      "     53       37.2320  0.0959\n",
      "     54       37.2320  0.0992\n",
      "     55       37.2320  0.0956\n",
      "     56       37.2320  0.0869\n",
      "     57       37.2320  0.0924\n",
      "     58       37.2320  0.0867\n",
      "     59       37.2320  0.0970\n",
      "     60       37.2320  0.0854\n",
      "     61       37.2320  0.0978\n",
      "     62       37.2320  0.0983\n",
      "     63       37.2320  0.1033\n",
      "     64       37.2320  0.0866\n",
      "     65       37.2320  0.0938\n",
      "     66       37.2320  0.0883\n",
      "     67       37.2320  0.0971\n",
      "     68       37.2320  0.0954\n",
      "     69       37.2320  0.0845\n",
      "     70       37.2320  0.0890\n",
      "     71       37.2320  0.0878\n",
      "     72       37.2320  0.0897\n",
      "     73       37.2320  0.0972\n",
      "     74       37.2320  0.0896\n",
      "     75       37.2320  0.0949\n",
      "     76       37.2320  0.1039\n",
      "     77       37.2320  0.0775\n",
      "     78       37.2320  0.0966\n",
      "     79       37.2320  0.0915\n",
      "     80       37.2320  0.1099\n",
      "     81       37.2320  0.0976\n",
      "     82       37.2320  0.0936\n",
      "     83       37.2320  0.0891\n",
      "     84       37.2320  0.1048\n",
      "     85       37.2320  0.0844\n",
      "     86       37.2320  0.0895\n",
      "     87       37.2320  0.0920\n",
      "     88       37.2320  0.0930\n",
      "     89       37.2320  0.0934\n",
      "     90       37.2320  0.0951\n",
      "     91       37.2320  0.0841\n",
      "     92       37.2320  0.0985\n",
      "     93       37.2320  0.0900\n",
      "     94       37.2320  0.0888\n",
      "     95       37.2320  0.0948\n",
      "     96       37.2320  0.1247\n",
      "     97       37.2320  0.0865\n",
      "     98       37.2320  0.0953\n",
      "     99       37.2320  0.0982\n",
      "    100       37.2320  0.1013\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0781\n",
      "      2       37.2320  0.1004\n",
      "      3       37.2320  0.0955\n",
      "      4       37.2320  0.0973\n",
      "      5       37.2320  0.0886\n",
      "      6       37.2320  0.0921\n",
      "      7       37.2320  0.0925\n",
      "      8       37.2320  0.0940\n",
      "      9       37.2320  0.0934\n",
      "     10       37.2320  0.1037\n",
      "     11       37.2320  0.0945\n",
      "     12       37.2320  0.0991\n",
      "     13       37.2320  0.0942\n",
      "     14       37.2320  0.0989\n",
      "     15       37.2320  0.1029\n",
      "     16       37.2320  0.1012\n",
      "     17       37.2320  0.0982\n",
      "     18       37.2320  0.0855\n",
      "     19       37.2320  0.0962\n",
      "     20       37.2320  0.0835\n",
      "     21       37.2320  0.0898\n",
      "     22       37.2320  0.0887\n",
      "     23       37.2320  0.0970\n",
      "     24       37.2320  0.0873\n",
      "     25       37.2320  0.0890\n",
      "     26       17.4384  0.0859\n",
      "     27        0.6449  0.0917\n",
      "     28        0.6204  0.0899\n",
      "     29        0.6185  0.0901\n",
      "     30        0.6175  0.0876\n",
      "     31        0.6167  0.0943\n",
      "     32        0.6161  0.0919\n",
      "     33        0.6156  0.0902\n",
      "     34        0.6152  0.0985\n",
      "     35        0.6149  0.1221\n",
      "     36        0.6147  0.1058\n",
      "     37        0.6144  0.1011\n",
      "     38        0.6142  0.0989\n",
      "     39        0.6140  0.0975\n",
      "     40        0.6139  0.0931\n",
      "     41        0.6138  0.0989\n",
      "     42        0.6137  0.0929\n",
      "     43        0.6136  0.0959\n",
      "     44        0.6135  0.0948\n",
      "     45        0.6134  0.0962\n",
      "     46        0.6134  0.0892\n",
      "     47        0.6133  0.0993\n",
      "     48        0.6133  0.0847\n",
      "     49        0.6132  0.1009\n",
      "     50        0.6132  0.0962\n",
      "     51        0.6132  0.0994\n",
      "     52        0.6131  0.1214\n",
      "     53        0.6131  0.0909\n",
      "     54        0.6131  0.1029\n",
      "     55        0.6131  0.0967\n",
      "     56        0.6131  0.1052\n",
      "     57        0.6130  0.0963\n",
      "     58        0.6130  0.1033\n",
      "     59        0.6130  0.1036\n",
      "     60        0.6130  0.1017\n",
      "     61        0.6130  0.1025\n",
      "     62        0.6130  0.0893\n",
      "     63        0.6130  0.0887\n",
      "     64        0.6130  0.0972\n",
      "     65        0.6130  0.0864\n",
      "     66        0.6130  0.0921\n",
      "     67        0.6130  0.1022\n",
      "     68        0.6130  0.0904\n",
      "     69        0.6130  0.1234\n",
      "     70        0.6130  0.0997\n",
      "     71        0.6130  0.0877\n",
      "     72        0.6130  0.0965\n",
      "     73        0.6130  0.0879\n",
      "     74        0.6130  0.1039\n",
      "     75        0.6130  0.0953\n",
      "     76        0.6130  0.0929\n",
      "     77        0.6130  0.0946\n",
      "     78        0.6130  0.0845\n",
      "     79        0.6130  0.0836\n",
      "     80        0.6130  0.0975\n",
      "     81        0.6130  0.0906\n",
      "     82        0.6130  0.0850\n",
      "     83        0.6130  0.0920\n",
      "     84        0.6130  0.0851\n",
      "     85        0.6130  0.0864\n",
      "     86        0.6130  0.0972\n",
      "     87        0.6130  0.0897\n",
      "     88        0.6130  0.0972\n",
      "     89        0.6130  0.0916\n",
      "     90        0.6130  0.0944\n",
      "     91        0.6130  0.0908\n",
      "     92        0.6130  0.0937\n",
      "     93        0.6130  0.0975\n",
      "     94        0.6130  0.0973\n",
      "     95        0.6130  0.1074\n",
      "     96        0.6130  0.0994\n",
      "     97        0.6130  0.0899\n",
      "     98        0.6130  0.0894\n",
      "     99        0.6130  0.1165\n",
      "    100        0.6130  0.0904\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(skorch_classifier, X, Y, cv = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10,) \n",
      "\n",
      "Results: [0.86206897 0.87931034 0.87719298 0.63157895 0.87719298 0.9122807\n",
      " 0.85964912 0.92857143 0.625      0.625     ] \n",
      "\n",
      "Mean: 0.8077845475758361\n",
      "Standard Deviation: 0.11990222917065621\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', results.shape, '\\n')\n",
    "print('Results:', results,'\\n')\n",
    "print('Mean:', results.mean())\n",
    "print('Standard Deviation:', results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_classifier_dropout(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # 30 -> 16 -> 16 -> 1\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight)\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dropout0(X)\n",
    "        \n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dropout1(X)\n",
    "        \n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "skorch_classifier = NeuralNetBinaryClassifier(module=torch_classifier_dropout, criterion=torch.nn.BCELoss,\n",
    "                                              optimizer=torch.optim.Adam, lr=0.001,\n",
    "                                              optimizer__weight_decay=0.0001, max_epochs=100, \n",
    "                                              batch_size=10, train_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:665: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_folds = np.zeros(n_samples, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\Mafeus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1820  3.1506\n",
      "      2       37.1820  0.1380\n",
      "      3       37.1820  0.0964\n",
      "      4       37.1820  0.0998\n",
      "      5       37.1820  0.1014\n",
      "      6       37.1820  0.1003\n",
      "      7       37.1820  0.1176\n",
      "      8       37.1820  0.1158\n",
      "      9       37.1820  0.1207\n",
      "     10       37.1820  0.1017\n",
      "     11       37.1820  0.1105\n",
      "     12       37.1820  0.1512\n",
      "     13       37.1820  0.1047\n",
      "     14       37.1820  0.1343\n",
      "     15       37.1820  0.1030\n",
      "     16       37.1820  0.1165\n",
      "     17       37.1820  0.1027\n",
      "     18       37.1820  0.1252\n",
      "     19       37.1820  0.0972\n",
      "     20       37.1820  0.1008\n",
      "     21       37.1820  0.1337\n",
      "     22       37.1820  0.0902\n",
      "     23       21.9331  0.1170\n",
      "     24        0.6119  0.1187\n",
      "     25        0.5371  0.1141\n",
      "     26        0.5032  0.1460\n",
      "     27        0.4920  0.1196\n",
      "     28        0.4754  0.0986\n",
      "     29        0.4800  0.1335\n",
      "     30        0.4506  0.1042\n",
      "     31        0.4743  0.1117\n",
      "     32        0.4315  0.1547\n",
      "     33        0.4145  0.1217\n",
      "     34        0.3978  0.1201\n",
      "     35        0.3848  0.1310\n",
      "     36        0.3834  0.1416\n",
      "     37        0.3694  0.1287\n",
      "     38        0.3473  0.1900\n",
      "     39        0.3679  0.1058\n",
      "     40        0.3482  0.1015\n",
      "     41        0.3305  0.0941\n",
      "     42        0.3282  0.1035\n",
      "     43        0.3386  0.1048\n",
      "     44        0.3140  0.0971\n",
      "     45        0.3137  0.0905\n",
      "     46        0.3039  0.0897\n",
      "     47        0.2816  0.1301\n",
      "     48        0.3013  0.1040\n",
      "     49        0.2824  0.0930\n",
      "     50        0.2737  0.0860\n",
      "     51        0.2799  0.1013\n",
      "     52        0.2962  0.0850\n",
      "     53        0.2782  0.0978\n",
      "     54        0.2743  0.0858\n",
      "     55        0.2920  0.0909\n",
      "     56        0.2721  0.0979\n",
      "     57        0.2371  0.0887\n",
      "     58        0.2698  0.0982\n",
      "     59        0.2569  0.0921\n",
      "     60        0.2464  0.0938\n",
      "     61        0.2481  0.1020\n",
      "     62        0.2647  0.1339\n",
      "     63        0.2327  0.1192\n",
      "     64        0.2331  0.0963\n",
      "     65        0.2517  0.1055\n",
      "     66        0.2457  0.1176\n",
      "     67        0.2388  0.1196\n",
      "     68        0.2410  0.1097\n",
      "     69        0.2222  0.0981\n",
      "     70        0.2363  0.0929\n",
      "     71        0.2211  0.1113\n",
      "     72        0.2557  0.1108\n",
      "     73        0.2373  0.0908\n",
      "     74        0.2680  0.1101\n",
      "     75        0.2503  0.0863\n",
      "     76        0.2412  0.1164\n",
      "     77        0.2424  0.1135\n",
      "     78        0.2064  0.0923\n",
      "     79        0.2278  0.0906\n",
      "     80        0.2273  0.0937\n",
      "     81        0.2231  0.0958\n",
      "     82        0.2312  0.0887\n",
      "     83        0.2361  0.0913\n",
      "     84        0.2349  0.1162\n",
      "     85        0.2620  0.1170\n",
      "     86        0.2256  0.1250\n",
      "     87        0.2142  0.1088\n",
      "     88        0.2283  0.1524\n",
      "     89        0.2291  0.0995\n",
      "     90        0.2269  0.1176\n",
      "     91        0.2131  0.1147\n",
      "     92        0.2183  0.1217\n",
      "     93        0.2293  0.1286\n",
      "     94        0.2364  0.1057\n",
      "     95        0.2139  0.1157\n",
      "     96        0.2311  0.1337\n",
      "     97        0.2496  0.1147\n",
      "     98        0.2335  0.1067\n",
      "     99        0.2243  0.1097\n",
      "    100        0.2052  0.1127\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1820  0.1416\n",
      "      2       37.1820  0.1636\n",
      "      3       37.1820  0.1277\n",
      "      4       37.1820  0.1127\n",
      "      5       37.1820  0.1107\n",
      "      6       37.1820  0.1067\n",
      "      7       37.1820  0.1217\n",
      "      8       37.1820  0.1077\n",
      "      9       37.1820  0.1127\n",
      "     10       37.1820  0.1187\n",
      "     11       37.1820  0.1060\n",
      "     12       37.1820  0.1067\n",
      "     13       37.1820  0.1057\n",
      "     14       37.1820  0.1067\n",
      "     15       37.1820  0.1117\n",
      "     16       37.1820  0.1107\n",
      "     17       37.1820  0.1077\n",
      "     18       37.1820  0.1147\n",
      "     19       37.1820  0.1267\n",
      "     20       37.1820  0.1157\n",
      "     21       37.1820  0.1187\n",
      "     22       37.1820  0.1067\n",
      "     23       37.1820  0.1179\n",
      "     24       37.1820  0.1100\n",
      "     25       37.1820  0.1117\n",
      "     26       15.1330  0.1040\n",
      "     27        0.5892  0.1129\n",
      "     28        0.5410  0.1187\n",
      "     29        0.5337  0.1222\n",
      "     30        0.5655  0.1267\n",
      "     31        0.5358  0.1057\n",
      "     32        0.5035  0.1087\n",
      "     33        0.4921  0.1237\n",
      "     34        0.4921  0.1157\n",
      "     35        0.4966  0.1346\n",
      "     36        0.5010  0.1067\n",
      "     37        0.4890  0.1008\n",
      "     38        0.4576  0.1257\n",
      "     39        0.4615  0.1117\n",
      "     40        0.4672  0.1177\n",
      "     41        0.4346  0.1526\n",
      "     42        0.4197  0.1147\n",
      "     43        0.4198  0.1207\n",
      "     44        0.4428  0.1077\n",
      "     45        0.4230  0.1071\n",
      "     46        0.4210  0.1107\n",
      "     47        0.3997  0.1357\n",
      "     48        0.3924  0.1177\n",
      "     49        0.3889  0.1077\n",
      "     50        0.4061  0.1197\n",
      "     51        0.3945  0.1097\n",
      "     52        0.3941  0.1306\n",
      "     53        0.3977  0.1111\n",
      "     54        0.3838  0.1347\n",
      "     55        0.3877  0.1177\n",
      "     56        0.3698  0.1117\n",
      "     57        0.3623  0.1217\n",
      "     58        0.3804  0.1032\n",
      "     59        0.3442  0.1299\n",
      "     60        0.3180  0.0962\n",
      "     61        0.3176  0.1185\n",
      "     62        0.3446  0.1319\n",
      "     63        0.3512  0.1100\n",
      "     64        0.3601  0.1086\n",
      "     65        0.3454  0.1097\n",
      "     66        0.3443  0.1207\n",
      "     67        0.2988  0.1279\n",
      "     68        0.3504  0.1183\n",
      "     69        0.3518  0.1143\n",
      "     70        0.3393  0.1172\n",
      "     71        0.3140  0.1039\n",
      "     72        0.3515  0.1197\n",
      "     73        0.3485  0.1630\n",
      "     74        0.3559  0.1326\n",
      "     75        0.2949  0.1277\n",
      "     76        0.2999  0.1166\n",
      "     77        0.3462  0.1180\n",
      "     78        0.3126  0.1353\n",
      "     79        0.3324  0.1012\n",
      "     80        0.3225  0.1185\n",
      "     81        0.3472  0.1191\n",
      "     82        0.3279  0.1203\n",
      "     83        0.2990  0.1607\n",
      "     84        0.3459  0.1162\n",
      "     85        0.3406  0.1243\n",
      "     86        0.3085  0.1002\n",
      "     87        0.3659  0.1030\n",
      "     88        0.3606  0.1033\n",
      "     89        0.3386  0.1185\n",
      "     90        0.3314  0.1034\n",
      "     91        0.3240  0.1271\n",
      "     92        0.3109  0.1295\n",
      "     93        0.3168  0.1290\n",
      "     94        0.3124  0.1000\n",
      "     95        0.3530  0.1052\n",
      "     96        0.3144  0.1185\n",
      "     97        0.3192  0.1059\n",
      "     98        0.3075  0.1671\n",
      "     99        0.3385  0.0983\n",
      "    100        0.3625  0.1033\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0992\n",
      "      2       37.3047  0.1187\n",
      "      3       37.3047  0.1137\n",
      "      4       37.3047  0.1566\n",
      "      5       37.3047  0.1431\n",
      "      6       37.3047  0.0975\n",
      "      7       37.3047  0.1291\n",
      "      8       37.3047  0.1396\n",
      "      9       37.3047  0.1178\n",
      "     10       37.3047  0.1003\n",
      "     11       37.3047  0.1026\n",
      "     12       37.3047  0.0992\n",
      "     13       37.3047  0.1347\n",
      "     14       37.3047  0.1164\n",
      "     15       37.3047  0.1515\n",
      "     16       37.3047  0.1229\n",
      "     17       37.3047  0.1250\n",
      "     18       37.3047  0.0984\n",
      "     19       37.3047  0.1020\n",
      "     20       37.3047  0.1002\n",
      "     21       37.3047  0.1521\n",
      "     22       37.3047  0.1163\n",
      "     23       37.3047  0.1146\n",
      "     24       37.3047  0.1024\n",
      "     25       37.3047  0.1171\n",
      "     26       37.3047  0.1083\n",
      "     27       37.3047  0.1121\n",
      "     28       37.3047  0.1020\n",
      "     29       37.3047  0.1166\n",
      "     30       37.3047  0.1131\n",
      "     31       12.3999  0.1147\n",
      "     32        0.6550  0.1031\n",
      "     33        0.6297  0.1108\n",
      "     34        0.6217  0.1148\n",
      "     35        0.6333  0.1024\n",
      "     36        0.6221  0.1175\n",
      "     37        0.6097  0.1473\n",
      "     38        0.5912  0.1198\n",
      "     39        0.6122  0.1596\n",
      "     40        0.5944  0.1145\n",
      "     41        0.5839  0.1636\n",
      "     42        0.5883  0.1233\n",
      "     43        0.5963  0.1442\n",
      "     44        0.5789  0.0992\n",
      "     45        0.5984  0.1646\n",
      "     46        0.5863  0.1006\n",
      "     47        0.5869  0.1202\n",
      "     48        0.5897  0.1267\n",
      "     49        0.5831  0.0991\n",
      "     50        0.5777  0.1342\n",
      "     51        0.5993  0.1217\n",
      "     52        0.5930  0.1155\n",
      "     53        0.5912  0.1516\n",
      "     54        0.5900  0.0974\n",
      "     55        0.5865  0.1390\n",
      "     56        0.5758  0.1205\n",
      "     57        0.5783  0.1476\n",
      "     58        0.5819  0.1137\n",
      "     59        0.5742  0.0941\n",
      "     60        0.5891  0.1306\n",
      "     61        0.5763  0.1267\n",
      "     62        0.5704  0.1336\n",
      "     63        0.5934  0.1098\n",
      "     64        0.5760  0.2669\n",
      "     65        0.6008  0.1040\n",
      "     66        0.5709  0.1034\n",
      "     67        0.5975  0.0970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     68        0.5901  0.0935\n",
      "     69        0.5824  0.1117\n",
      "     70        0.5919  0.1080\n",
      "     71        0.5698  0.1072\n",
      "     72        0.5897  0.0955\n",
      "     73        0.5720  0.1103\n",
      "     74        0.5810  0.0950\n",
      "     75        0.5799  0.1102\n",
      "     76        0.5740  0.0976\n",
      "     77        0.5920  0.1027\n",
      "     78        0.5844  0.1449\n",
      "     79        0.5892  0.0972\n",
      "     80        0.5718  0.1006\n",
      "     81        0.5835  0.0863\n",
      "     82        0.5770  0.0855\n",
      "     83        0.5762  0.0897\n",
      "     84        0.5829  0.0980\n",
      "     85        0.5848  0.0865\n",
      "     86        0.5831  0.0961\n",
      "     87        0.5918  0.0976\n",
      "     88        0.5874  0.1046\n",
      "     89        0.5867  0.1138\n",
      "     90        0.5744  0.0901\n",
      "     91        0.5855  0.0975\n",
      "     92        0.5878  0.1086\n",
      "     93        0.5859  0.1053\n",
      "     94        0.5842  0.0996\n",
      "     95        0.5909  0.1177\n",
      "     96        0.5892  0.1075\n",
      "     97        0.5734  0.1092\n",
      "     98        0.5927  0.1022\n",
      "     99        0.5798  0.1076\n",
      "    100        0.5860  0.1177\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0935\n",
      "      2       37.3047  0.1120\n",
      "      3       37.3047  0.1102\n",
      "      4       37.3047  0.1045\n",
      "      5       37.3047  0.1054\n",
      "      6       37.3047  0.0978\n",
      "      7       37.3047  0.1016\n",
      "      8       37.3047  0.0924\n",
      "      9       37.3047  0.0935\n",
      "     10       37.3047  0.1006\n",
      "     11       37.3047  0.0995\n",
      "     12       37.3047  0.1021\n",
      "     13       37.3047  0.1078\n",
      "     14       37.3047  0.0953\n",
      "     15       37.3047  0.1057\n",
      "     16       37.3047  0.1058\n",
      "     17       37.3047  0.1091\n",
      "     18       37.3047  0.1284\n",
      "     19       37.3047  0.0936\n",
      "     20       37.3047  0.1074\n",
      "     21       37.3047  0.0859\n",
      "     22       29.5577  0.0918\n",
      "     23        2.5234  0.1147\n",
      "     24        0.6466  0.0963\n",
      "     25        0.6036  0.1057\n",
      "     26        0.5630  0.1011\n",
      "     27        0.5558  0.1329\n",
      "     28        0.5324  0.1152\n",
      "     29        0.5195  0.1204\n",
      "     30        0.5211  0.1055\n",
      "     31        0.5232  0.1277\n",
      "     32        0.5292  0.1008\n",
      "     33        0.5071  0.0856\n",
      "     34        0.5163  0.1165\n",
      "     35        0.5160  0.0843\n",
      "     36        0.5230  0.0931\n",
      "     37        0.4736  0.1071\n",
      "     38        0.4980  0.1037\n",
      "     39        0.5055  0.0884\n",
      "     40        0.5001  0.1059\n",
      "     41        0.4945  0.0959\n",
      "     42        0.5051  0.1041\n",
      "     43        0.4875  0.1124\n",
      "     44        0.4958  0.0975\n",
      "     45        0.4735  0.1091\n",
      "     46        0.4681  0.1110\n",
      "     47        0.4656  0.0936\n",
      "     48        0.4688  0.1167\n",
      "     49        0.4817  0.0871\n",
      "     50        0.4768  0.1019\n",
      "     51        0.4709  0.0859\n",
      "     52        0.4691  0.0959\n",
      "     53        0.4650  0.1156\n",
      "     54        0.4234  0.1159\n",
      "     55        0.4281  0.0929\n",
      "     56        0.4509  0.0933\n",
      "     57        0.4096  0.1150\n",
      "     58        0.4299  0.0986\n",
      "     59        0.4296  0.1118\n",
      "     60        0.4176  0.1035\n",
      "     61        0.4304  0.1089\n",
      "     62        0.4010  0.1022\n",
      "     63        0.4014  0.1200\n",
      "     64        0.3933  0.1065\n",
      "     65        0.3932  0.1120\n",
      "     66        0.3844  0.1373\n",
      "     67        0.3828  0.0901\n",
      "     68        0.3771  0.0975\n",
      "     69        0.4062  0.1034\n",
      "     70        0.3875  0.0949\n",
      "     71        0.3762  0.0936\n",
      "     72        0.3728  0.0898\n",
      "     73        0.3564  0.1154\n",
      "     74        0.3862  0.1170\n",
      "     75        0.4071  0.0968\n",
      "     76        0.3882  0.0850\n",
      "     77        0.3432  0.1087\n",
      "     78        0.3708  0.0960\n",
      "     79        0.3537  0.1137\n",
      "     80        0.3799  0.0978\n",
      "     81        0.3882  0.0944\n",
      "     82        0.3583  0.0929\n",
      "     83        0.3764  0.1016\n",
      "     84        0.3382  0.1050\n",
      "     85        0.3598  0.0986\n",
      "     86        0.3755  0.0951\n",
      "     87        0.3732  0.0889\n",
      "     88        0.3569  0.1178\n",
      "     89        0.3269  0.1138\n",
      "     90        0.3720  0.1018\n",
      "     91        0.3732  0.0996\n",
      "     92        0.3635  0.1028\n",
      "     93        0.3285  0.0943\n",
      "     94        0.3643  0.0903\n",
      "     95        0.3589  0.1082\n",
      "     96        0.3402  0.1146\n",
      "     97        0.3436  0.0969\n",
      "     98        0.3449  0.0865\n",
      "     99        0.3891  0.0957\n",
      "    100        0.3428  0.0874\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0937\n",
      "      2       37.3047  0.1088\n",
      "      3       37.3047  0.0843\n",
      "      4       37.3047  0.1029\n",
      "      5       37.3047  0.0987\n",
      "      6       37.3047  0.0844\n",
      "      7       37.3047  0.0854\n",
      "      8       37.3047  0.0867\n",
      "      9       37.3047  0.1001\n",
      "     10       37.3047  0.0927\n",
      "     11       37.3047  0.0905\n",
      "     12       37.3047  0.1043\n",
      "     13       37.3047  0.1115\n",
      "     14       37.3047  0.1188\n",
      "     15       37.3047  0.0902\n",
      "     16       37.3047  0.0865\n",
      "     17       37.3047  0.0931\n",
      "     18       37.3047  0.0941\n",
      "     19       37.3047  0.0913\n",
      "     20       37.3047  0.0952\n",
      "     21       37.3047  0.1115\n",
      "     22       37.3047  0.0845\n",
      "     23       37.3049  0.1101\n",
      "     24       37.3047  0.0956\n",
      "     25       37.3047  0.0849\n",
      "     26       37.3047  0.0939\n",
      "     27       37.3047  0.0961\n",
      "     28       36.8000  0.1047\n",
      "     29        3.5846  0.0923\n",
      "     30        0.6479  0.1130\n",
      "     31        0.6285  0.0921\n",
      "     32        0.5998  0.0992\n",
      "     33        0.5800  0.1219\n",
      "     34        0.5507  0.0969\n",
      "     35        0.5441  0.1023\n",
      "     36        0.4971  0.1186\n",
      "     37        0.4913  0.1061\n",
      "     38        0.4811  0.0966\n",
      "     39        0.4626  0.1057\n",
      "     40        0.4493  0.0897\n",
      "     41        0.4059  0.1028\n",
      "     42        0.4005  0.0879\n",
      "     43        0.4160  0.1028\n",
      "     44        0.4042  0.0975\n",
      "     45        0.3548  0.0869\n",
      "     46        0.3554  0.0899\n",
      "     47        0.3874  0.1134\n",
      "     48        0.3392  0.0851\n",
      "     49        0.3567  0.1033\n",
      "     50        0.3316  0.1304\n",
      "     51        0.3487  0.0924\n",
      "     52        0.3267  0.0859\n",
      "     53        0.3460  0.0895\n",
      "     54        0.3395  0.1013\n",
      "     55        0.3314  0.1035\n",
      "     56        0.2985  0.1127\n",
      "     57        0.3201  0.0907\n",
      "     58        0.3023  0.0982\n",
      "     59        0.3306  0.0892\n",
      "     60        0.2977  0.0977\n",
      "     61        0.2906  0.1007\n",
      "     62        0.3023  0.0905\n",
      "     63        0.2993  0.1056\n",
      "     64        0.3031  0.0858\n",
      "     65        0.3244  0.0927\n",
      "     66        0.3024  0.1257\n",
      "     67        0.2859  0.0867\n",
      "     68        0.2619  0.0989\n",
      "     69        0.3201  0.0939\n",
      "     70        0.2787  0.1008\n",
      "     71        0.2722  0.0932\n",
      "     72        0.3176  0.0896\n",
      "     73        0.2765  0.0906\n",
      "     74        0.2656  0.0938\n",
      "     75        0.2989  0.0991\n",
      "     76        0.2661  0.0876\n",
      "     77        0.2797  0.0848\n",
      "     78        0.2748  0.1006\n",
      "     79        0.2760  0.1067\n",
      "     80        0.2994  0.0958\n",
      "     81        0.2617  0.0970\n",
      "     82        0.2831  0.1047\n",
      "     83        0.2838  0.0977\n",
      "     84        0.2619  0.1067\n",
      "     85        0.2600  0.1007\n",
      "     86        0.2551  0.1117\n",
      "     87        0.3215  0.0987\n",
      "     88        0.2788  0.1027\n",
      "     89        0.2608  0.0990\n",
      "     90        0.2559  0.1007\n",
      "     91        0.2618  0.0990\n",
      "     92        0.2638  0.0997\n",
      "     93        0.2741  0.0979\n",
      "     94        0.2764  0.1087\n",
      "     95        0.2810  0.0977\n",
      "     96        0.2486  0.1017\n",
      "     97        0.2557  0.1011\n",
      "     98        0.2590  0.1187\n",
      "     99        0.2411  0.1067\n",
      "    100        0.3067  0.1087\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0957\n",
      "      2       37.3047  0.1097\n",
      "      3       37.3047  0.1097\n",
      "      4       37.3047  0.0948\n",
      "      5       37.3047  0.0958\n",
      "      6       37.3047  0.1018\n",
      "      7       37.3047  0.0978\n",
      "      8       37.3047  0.1027\n",
      "      9       37.3047  0.0997\n",
      "     10       37.3047  0.1007\n",
      "     11       37.3047  0.1007\n",
      "     12       37.3047  0.0967\n",
      "     13       37.3047  0.0967\n",
      "     14       37.3047  0.1147\n",
      "     15       37.3047  0.0951\n",
      "     16       37.3047  0.1007\n",
      "     17       37.3047  0.0957\n",
      "     18       37.3047  0.0948\n",
      "     19       37.3047  0.1048\n",
      "     20       37.3047  0.0997\n",
      "     21       37.3047  0.0977\n",
      "     22       37.3047  0.1008\n",
      "     23       37.3047  0.0958\n",
      "     24       37.3047  0.0958\n",
      "     25       37.3047  0.0968\n",
      "     26       37.3047  0.1017\n",
      "     27       37.3047  0.0947\n",
      "     28       37.3047  0.1047\n",
      "     29       17.5889  0.0970\n",
      "     30        0.6519  0.0997\n",
      "     31        0.6404  0.1007\n",
      "     32        0.6266  0.1027\n",
      "     33        0.6254  0.1008\n",
      "     34        0.6236  0.0997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35        0.6037  0.1057\n",
      "     36        0.5925  0.0978\n",
      "     37        0.6045  0.1007\n",
      "     38        0.5844  0.1167\n",
      "     39        0.5740  0.1017\n",
      "     40        0.5710  0.0970\n",
      "     41        0.5751  0.1047\n",
      "     42        0.5699  0.0987\n",
      "     43        0.5631  0.1017\n",
      "     44        0.5747  0.0997\n",
      "     45        0.5737  0.0977\n",
      "     46        0.5657  0.0957\n",
      "     47        0.5657  0.0957\n",
      "     48        0.5837  0.0981\n",
      "     49        0.5604  0.0967\n",
      "     50        0.5508  0.1117\n",
      "     51        0.5642  0.1077\n",
      "     52        0.5724  0.1027\n",
      "     53        0.5540  0.0967\n",
      "     54        0.5661  0.0876\n",
      "     55        0.5647  0.1081\n",
      "     56        0.5628  0.1104\n",
      "     57        0.5603  0.0880\n",
      "     58        0.5617  0.0952\n",
      "     59        0.5611  0.1005\n",
      "     60        0.5531  0.1050\n",
      "     61        0.5507  0.0881\n",
      "     62        0.5475  0.0942\n",
      "     63        0.5708  0.1194\n",
      "     64        0.5587  0.1126\n",
      "     65        0.5542  0.1070\n",
      "     66        0.5462  0.0955\n",
      "     67        0.5586  0.1095\n",
      "     68        0.5582  0.1109\n",
      "     69        0.5696  0.0886\n",
      "     70        0.5488  0.1061\n",
      "     71        0.5524  0.0967\n",
      "     72        0.5604  0.1014\n",
      "     73        0.5509  0.0972\n",
      "     74        0.5559  0.0972\n",
      "     75        0.5445  0.0987\n",
      "     76        0.5601  0.0986\n",
      "     77        0.5578  0.0982\n",
      "     78        0.5496  0.0867\n",
      "     79        0.5541  0.0904\n",
      "     80        0.5446  0.0967\n",
      "     81        0.5519  0.1076\n",
      "     82        0.5407  0.1103\n",
      "     83        0.5519  0.1029\n",
      "     84        0.5482  0.1012\n",
      "     85        0.5460  0.0882\n",
      "     86        0.5299  0.0863\n",
      "     87        0.5377  0.0967\n",
      "     88        0.5410  0.1005\n",
      "     89        0.5465  0.1028\n",
      "     90        0.5424  0.1080\n",
      "     91        0.5403  0.1251\n",
      "     92        0.5282  0.1081\n",
      "     93        0.5425  0.1088\n",
      "     94        0.5323  0.1402\n",
      "     95        0.5289  0.1468\n",
      "     96        0.5287  0.0978\n",
      "     97        0.5277  0.1144\n",
      "     98        0.5275  0.0935\n",
      "     99        0.5274  0.0914\n",
      "    100        0.5391  0.0988\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0949\n",
      "      2       37.3047  0.0983\n",
      "      3       37.3047  0.0972\n",
      "      4       37.3047  0.1003\n",
      "      5       37.3047  0.1099\n",
      "      6       37.3047  0.0894\n",
      "      7       37.3047  0.1131\n",
      "      8       37.3047  0.0928\n",
      "      9       37.3047  0.0870\n",
      "     10       37.3047  0.1039\n",
      "     11       37.3047  0.1044\n",
      "     12       37.3047  0.1186\n",
      "     13       37.3047  0.1022\n",
      "     14       37.3047  0.1144\n",
      "     15       37.3047  0.1001\n",
      "     16       37.3047  0.0988\n",
      "     17       37.3047  0.0945\n",
      "     18       37.3047  0.0898\n",
      "     19       37.3047  0.0926\n",
      "     20       37.3047  0.1001\n",
      "     21       37.3047  0.0838\n",
      "     22       37.3047  0.0960\n",
      "     23       28.2154  0.0977\n",
      "     24        0.7438  0.0967\n",
      "     25        0.5752  0.1008\n",
      "     26        0.5888  0.0985\n",
      "     27        0.5714  0.0910\n",
      "     28        0.5357  0.1019\n",
      "     29        0.5563  0.0949\n",
      "     30        0.5056  0.1052\n",
      "     31        0.5040  0.0950\n",
      "     32        0.5066  0.0965\n",
      "     33        0.5078  0.1001\n",
      "     34        0.4997  0.1168\n",
      "     35        0.4717  0.0916\n",
      "     36        0.4874  0.1020\n",
      "     37        0.4609  0.0973\n",
      "     38        0.4676  0.1032\n",
      "     39        0.4544  0.1074\n",
      "     40        0.4216  0.1001\n",
      "     41        0.4167  0.1108\n",
      "     42        0.3905  0.1010\n",
      "     43        0.3560  0.0966\n",
      "     44        0.4184  0.0870\n",
      "     45        0.3809  0.0847\n",
      "     46        0.3660  0.0985\n",
      "     47        0.3365  0.1231\n",
      "     48        0.4074  0.0985\n",
      "     49        0.3409  0.1085\n",
      "     50        0.3166  0.0981\n",
      "     51        0.3323  0.1113\n",
      "     52        0.3135  0.0959\n",
      "     53        0.3425  0.0962\n",
      "     54        0.3328  0.0904\n",
      "     55        0.2985  0.0957\n",
      "     56        0.3256  0.0937\n",
      "     57        0.3246  0.0852\n",
      "     58        0.3124  0.0878\n",
      "     59        0.3312  0.0889\n",
      "     60        0.2946  0.1143\n",
      "     61        0.3278  0.0902\n",
      "     62        0.2930  0.0916\n",
      "     63        0.2997  0.1122\n",
      "     64        0.2993  0.1300\n",
      "     65        0.2840  0.1003\n",
      "     66        0.2828  0.0882\n",
      "     67        0.2905  0.0965\n",
      "     68        0.3068  0.1078\n",
      "     69        0.2751  0.0943\n",
      "     70        0.2959  0.0893\n",
      "     71        0.2795  0.1139\n",
      "     72        0.2925  0.0942\n",
      "     73        0.2666  0.1119\n",
      "     74        0.2847  0.1020\n",
      "     75        0.2482  0.1079\n",
      "     76        0.2693  0.0996\n",
      "     77        0.2583  0.1145\n",
      "     78        0.2587  0.1118\n",
      "     79        0.2352  0.1114\n",
      "     80        0.2541  0.0981\n",
      "     81        0.2547  0.1074\n",
      "     82        0.2393  0.0924\n",
      "     83        0.2768  0.0884\n",
      "     84        0.2593  0.0847\n",
      "     85        0.2740  0.1034\n",
      "     86        0.2710  0.1077\n",
      "     87        0.2247  0.0993\n",
      "     88        0.2256  0.1084\n",
      "     89        0.2797  0.1055\n",
      "     90        0.2228  0.1106\n",
      "     91        0.2485  0.1038\n",
      "     92        0.2324  0.1047\n",
      "     93        0.2350  0.1040\n",
      "     94        0.2541  0.1070\n",
      "     95        0.2438  0.1010\n",
      "     96        0.2446  0.1021\n",
      "     97        0.2085  0.0873\n",
      "     98        0.2346  0.1014\n",
      "     99        0.2197  0.0897\n",
      "    100        0.2275  0.0928\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0937\n",
      "      2       37.2320  0.1083\n",
      "      3       37.2320  0.0884\n",
      "      4       37.2320  0.1129\n",
      "      5       37.2320  0.0905\n",
      "      6       37.2320  0.1020\n",
      "      7       37.2320  0.1001\n",
      "      8       37.2320  0.1115\n",
      "      9       37.2320  0.0971\n",
      "     10       37.2320  0.1351\n",
      "     11       37.2320  0.1226\n",
      "     12       37.2320  0.0945\n",
      "     13       37.2320  0.1071\n",
      "     14       37.2320  0.0961\n",
      "     15       37.2320  0.0937\n",
      "     16       37.2320  0.1030\n",
      "     17       37.2320  0.0890\n",
      "     18       37.2320  0.1061\n",
      "     19       37.2320  0.0933\n",
      "     20       37.2320  0.0910\n",
      "     21       37.2320  0.1072\n",
      "     22       37.2320  0.0915\n",
      "     23       37.2320  0.1126\n",
      "     24       37.2320  0.0957\n",
      "     25       37.2320  0.0914\n",
      "     26       20.2563  0.1068\n",
      "     27        0.6089  0.0911\n",
      "     28        0.5808  0.0982\n",
      "     29        0.6082  0.1158\n",
      "     30        0.5693  0.1051\n",
      "     31        0.5672  0.1063\n",
      "     32        0.5640  0.0949\n",
      "     33        0.5523  0.0974\n",
      "     34        0.5697  0.0989\n",
      "     35        0.5754  0.1006\n",
      "     36        0.5611  0.0967\n",
      "     37        0.5576  0.0850\n",
      "     38        0.5685  0.0995\n",
      "     39        0.5573  0.1058\n",
      "     40        0.5631  0.0971\n",
      "     41        0.5432  0.0859\n",
      "     42        0.5406  0.1035\n",
      "     43        0.5656  0.0927\n",
      "     44        0.5525  0.0952\n",
      "     45        0.5464  0.0929\n",
      "     46        0.5602  0.1111\n",
      "     47        0.5488  0.0902\n",
      "     48        0.5524  0.1062\n",
      "     49        0.5475  0.0901\n",
      "     50        0.5510  0.0985\n",
      "     51        0.5559  0.0921\n",
      "     52        0.5480  0.0844\n",
      "     53        0.5487  0.0876\n",
      "     54        0.5491  0.0986\n",
      "     55        0.5545  0.0885\n",
      "     56        0.5412  0.1072\n",
      "     57        0.5596  0.0981\n",
      "     58        0.5483  0.0977\n",
      "     59        0.5455  0.1081\n",
      "     60        0.5463  0.0920\n",
      "     61        0.5426  0.1002\n",
      "     62        0.5341  0.1023\n",
      "     63        0.5216  0.1342\n",
      "     64        0.5448  0.1175\n",
      "     65        0.5425  0.1012\n",
      "     66        0.5347  0.0983\n",
      "     67        0.5283  0.0973\n",
      "     68        0.5247  0.0911\n",
      "     69        0.5362  0.1131\n",
      "     70        0.5233  0.0930\n",
      "     71        0.5276  0.1127\n",
      "     72        0.5235  0.0985\n",
      "     73        0.5246  0.1064\n",
      "     74        0.5142  0.0926\n",
      "     75        0.5246  0.0900\n",
      "     76        0.5312  0.0953\n",
      "     77        0.5063  0.0982\n",
      "     78        0.5213  0.0911\n",
      "     79        0.5243  0.1177\n",
      "     80        0.5075  0.0845\n",
      "     81        0.5154  0.1002\n",
      "     82        0.5205  0.1042\n",
      "     83        0.5100  0.0889\n",
      "     84        0.4960  0.0978\n",
      "     85        0.5022  0.1018\n",
      "     86        0.5043  0.0979\n",
      "     87        0.4957  0.0972\n",
      "     88        0.5139  0.1031\n",
      "     89        0.5089  0.0915\n",
      "     90        0.5231  0.1250\n",
      "     91        0.4962  0.1078\n",
      "     92        0.4891  0.0974\n",
      "     93        0.5208  0.1182\n",
      "     94        0.4956  0.1138\n",
      "     95        0.5034  0.1056\n",
      "     96        0.5005  0.0850\n",
      "     97        0.5143  0.1029\n",
      "     98        0.4899  0.0859\n",
      "     99        0.4728  0.1034\n",
      "    100        0.4930  0.1190\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.1249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2       37.2320  0.1154\n",
      "      3       37.2320  0.0889\n",
      "      4       37.2320  0.1125\n",
      "      5       37.2320  0.0979\n",
      "      6       37.2320  0.0990\n",
      "      7       37.2320  0.0850\n",
      "      8       37.2320  0.0910\n",
      "      9       37.2320  0.1181\n",
      "     10       37.2320  0.0908\n",
      "     11       37.2320  0.1057\n",
      "     12       37.2320  0.1086\n",
      "     13       37.2320  0.0877\n",
      "     14       37.2320  0.0855\n",
      "     15       37.2320  0.0885\n",
      "     16       37.2320  0.0871\n",
      "     17       37.2320  0.1019\n",
      "     18       37.2320  0.0997\n",
      "     19       37.2320  0.0882\n",
      "     20       37.2320  0.0865\n",
      "     21       37.2320  0.1117\n",
      "     22       24.5916  0.0937\n",
      "     23        0.6250  0.0867\n",
      "     24        0.5348  0.0875\n",
      "     25        0.5108  0.1031\n",
      "     26        0.4838  0.1140\n",
      "     27        0.5033  0.1201\n",
      "     28        0.4696  0.1013\n",
      "     29        0.4502  0.0947\n",
      "     30        0.4605  0.0905\n",
      "     31        0.4226  0.1150\n",
      "     32        0.4344  0.0906\n",
      "     33        0.3732  0.1095\n",
      "     34        0.4131  0.0965\n",
      "     35        0.4179  0.0856\n",
      "     36        0.3938  0.1076\n",
      "     37        0.3879  0.0933\n",
      "     38        0.3705  0.1061\n",
      "     39        0.3926  0.1078\n",
      "     40        0.3276  0.0968\n",
      "     41        0.3331  0.1076\n",
      "     42        0.3409  0.1132\n",
      "     43        0.3718  0.1115\n",
      "     44        0.3339  0.1073\n",
      "     45        0.3355  0.1063\n",
      "     46        0.2856  0.1005\n",
      "     47        0.2956  0.1052\n",
      "     48        0.2810  0.1048\n",
      "     49        0.3145  0.1021\n",
      "     50        0.3277  0.0977\n",
      "     51        0.2794  0.1103\n",
      "     52        0.2960  0.0960\n",
      "     53        0.2550  0.0855\n",
      "     54        0.2892  0.0991\n",
      "     55        0.2554  0.0890\n",
      "     56        0.2994  0.1144\n",
      "     57        0.2673  0.0869\n",
      "     58        0.2811  0.1144\n",
      "     59        0.2914  0.0977\n",
      "     60        0.2630  0.1078\n",
      "     61        0.2743  0.1136\n",
      "     62        0.2567  0.1043\n",
      "     63        0.2357  0.1031\n",
      "     64        0.2641  0.1013\n",
      "     65        0.2213  0.0842\n",
      "     66        0.2407  0.1146\n",
      "     67        0.2534  0.0993\n",
      "     68        0.2453  0.0912\n",
      "     69        0.2306  0.0970\n",
      "     70        0.2448  0.0876\n",
      "     71        0.2383  0.1129\n",
      "     72        0.2277  0.0868\n",
      "     73        0.2096  0.0881\n",
      "     74        0.2690  0.0893\n",
      "     75        0.2312  0.0988\n",
      "     76        0.2602  0.0891\n",
      "     77        0.2175  0.1281\n",
      "     78        0.2349  0.0869\n",
      "     79        0.2395  0.0987\n",
      "     80        0.2327  0.1005\n",
      "     81        0.3218  0.0904\n",
      "     82        0.2452  0.0941\n",
      "     83        0.2188  0.0884\n",
      "     84        0.2249  0.1032\n",
      "     85        0.2681  0.0912\n",
      "     86        0.2082  0.0969\n",
      "     87        0.2350  0.0911\n",
      "     88        0.2176  0.1135\n",
      "     89        0.2413  0.1094\n",
      "     90        0.2022  0.1114\n",
      "     91        0.2194  0.0901\n",
      "     92        0.2330  0.1153\n",
      "     93        0.2098  0.1019\n",
      "     94        0.2517  0.0940\n",
      "     95        0.2345  0.0921\n",
      "     96        0.1874  0.1047\n",
      "     97        0.2133  0.1029\n",
      "     98        0.2183  0.1022\n",
      "     99        0.2178  0.0844\n",
      "    100        0.2273  0.0978\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0938\n",
      "      2       37.2320  0.0998\n",
      "      3       37.2320  0.0973\n",
      "      4       37.2320  0.0986\n",
      "      5       37.2320  0.0841\n",
      "      6       37.2320  0.1008\n",
      "      7       37.2320  0.0952\n",
      "      8       37.2320  0.1252\n",
      "      9       37.2320  0.1253\n",
      "     10       37.2320  0.0888\n",
      "     11       37.2320  0.0883\n",
      "     12       37.2320  0.0870\n",
      "     13       37.2320  0.0901\n",
      "     14       37.2320  0.1010\n",
      "     15       37.2320  0.0934\n",
      "     16       37.2320  0.0930\n",
      "     17       37.2320  0.0923\n",
      "     18       37.2320  0.0911\n",
      "     19       37.2320  0.1011\n",
      "     20       37.2320  0.1000\n",
      "     21       37.2320  0.1090\n",
      "     22       37.2320  0.0991\n",
      "     23       37.2320  0.0936\n",
      "     24       37.2320  0.0995\n",
      "     25       37.2320  0.1017\n",
      "     26       37.2320  0.0871\n",
      "     27       37.2320  0.0920\n",
      "     28       24.0476  0.1263\n",
      "     29        0.6236  0.0890\n",
      "     30        0.6650  0.0999\n",
      "     31        0.5604  0.0958\n",
      "     32        0.5448  0.1016\n",
      "     33        0.5416  0.0891\n",
      "     34        0.5484  0.1030\n",
      "     35        0.5468  0.0919\n",
      "     36        0.5297  0.0973\n",
      "     37        0.5149  0.1098\n",
      "     38        0.5216  0.0885\n",
      "     39        0.5183  0.0844\n",
      "     40        0.5224  0.0994\n",
      "     41        0.5186  0.0950\n",
      "     42        0.5181  0.1076\n",
      "     43        0.4808  0.1012\n",
      "     44        0.4738  0.0997\n",
      "     45        0.4627  0.1295\n",
      "     46        0.4679  0.0923\n",
      "     47        0.4388  0.0847\n",
      "     48        0.4722  0.0941\n",
      "     49        0.4140  0.0973\n",
      "     50        0.3906  0.1313\n",
      "     51        0.3941  0.0924\n",
      "     52        0.3572  0.1026\n",
      "     53        0.3727  0.1037\n",
      "     54        0.3591  0.1096\n",
      "     55        0.3749  0.0967\n",
      "     56        0.3741  0.0948\n",
      "     57        0.3610  0.0948\n",
      "     58        0.3919  0.0958\n",
      "     59        0.3517  0.1007\n",
      "     60        0.3351  0.0990\n",
      "     61        0.3401  0.0998\n",
      "     62        0.3450  0.1008\n",
      "     63        0.3226  0.1008\n",
      "     64        0.3200  0.0978\n",
      "     65        0.3312  0.0998\n",
      "     66        0.3243  0.0988\n",
      "     67        0.3226  0.1027\n",
      "     68        0.3139  0.1037\n",
      "     69        0.3189  0.1000\n",
      "     70        0.3175  0.0997\n",
      "     71        0.2975  0.0991\n",
      "     72        0.2823  0.0997\n",
      "     73        0.2756  0.1007\n",
      "     74        0.2974  0.1018\n",
      "     75        0.2932  0.1018\n",
      "     76        0.2982  0.0978\n",
      "     77        0.2889  0.0997\n",
      "     78        0.2745  0.1107\n",
      "     79        0.3183  0.1028\n",
      "     80        0.2822  0.0978\n",
      "     81        0.2775  0.1008\n",
      "     82        0.2719  0.0987\n",
      "     83        0.2955  0.1067\n",
      "     84        0.2839  0.0997\n",
      "     85        0.2622  0.1007\n",
      "     86        0.2827  0.1007\n",
      "     87        0.2873  0.0981\n",
      "     88        0.2714  0.0987\n",
      "     89        0.2944  0.0997\n",
      "     90        0.2767  0.0997\n",
      "     91        0.2707  0.0977\n",
      "     92        0.2962  0.1017\n",
      "     93        0.2508  0.1021\n",
      "     94        0.2743  0.1011\n",
      "     95        0.2720  0.0972\n",
      "     96        0.2672  0.1107\n",
      "     97        0.2390  0.1018\n",
      "     98        0.2711  0.1047\n",
      "     99        0.2832  0.0990\n",
      "    100        0.2532  0.1007\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(skorch_classifier, X, Y, cv = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10,) \n",
      "\n",
      "Results: [0.84482759 0.81034483 0.63157895 0.85964912 0.87719298 0.57894737\n",
      " 0.8245614  0.73214286 0.91071429 0.85714286] \n",
      "\n",
      "Mean: 0.7927102238354506\n",
      "Standard Deviation: 0.10443305244509003\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', results.shape, '\\n')\n",
    "print('Results:', results,'\\n')\n",
    "print('Mean:', results.mean())\n",
    "print('Standard Deviation:', results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
